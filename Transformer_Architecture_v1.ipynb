{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MessiNN/chatbot-transformer-Early_Stage-/blob/master/Transformer_Architecture_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import itertools\n",
        "import pandas as pd\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device_cpu = torch.device(\"cpu\")\n",
        "\n",
        "save_dir = os.path.join(\"/content/drive/MyDrive\", \"data\", \"save\")\n",
        "\n",
        "PAD_token = 0\n",
        "SOS_token = 1\n",
        "EOS_token = 2\n",
        "\n",
        "MAX_LENGTH = 50\n",
        "MIN_COUNT = 15"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOO0zM6eym59",
        "outputId": "bad7ac7d-4a49-48ce-f87e-f37fa5fe8626"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Library:\n",
        "    def __init__(self):\n",
        "        self.name = \"Dataset\"\n",
        "        self.trimmed = False\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def trim(self, min_count):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "\n",
        "        keep_words = []\n",
        "\n",
        "        for k, v in self.word2count.items():\n",
        "            if v >= min_count:\n",
        "                keep_words.append(k)\n",
        "\n",
        "\n",
        "        print(len(keep_words), len(self.word2index), len(keep_words), len(self.word2index))\n",
        "        print('keep_words {} / {} = {:.4f}'.format(\n",
        "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
        "        ))\n",
        "\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3\n",
        "\n",
        "        for word in keep_words:\n",
        "            self.addWord(word)\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def readVocs(datafile):\n",
        "    pairs = []\n",
        "    df = pd.read_parquet(datafile)\n",
        "    questions = df['question'].tolist()\n",
        "    responses = df['response'].tolist()\n",
        "    for question, response in zip(questions, responses):\n",
        "      question = normalizeString(question)\n",
        "      response = normalizeString(response)\n",
        "      pair = [question, response]\n",
        "      pairs.append(pair)\n",
        "    return pairs\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "def loadPrepareData(datafile, save_dir):\n",
        "    libra = Library()\n",
        "    pairs = readVocs(datafile)\n",
        "    pairs = filterPairs(pairs)\n",
        "    for pair in pairs:\n",
        "        libra.addSentence(pair[0])\n",
        "        libra.addSentence(pair[1])\n",
        "    return libra, pairs\n",
        "\n",
        "\n",
        "def trimRareWords(libra, pairs, MIN_COUNT):\n",
        "    libra.trim(MIN_COUNT)\n",
        "    keep_pairs = []\n",
        "    for pair in pairs:\n",
        "        input_sentence = pair[0]\n",
        "        output_sentence = pair[1]\n",
        "        keep_input = True\n",
        "        keep_output = True\n",
        "        for word in input_sentence.split(' '):\n",
        "            if word not in libra.word2index:\n",
        "                keep_input = False\n",
        "                break\n",
        "        for word in output_sentence.split(' '):\n",
        "            if word not in libra.word2index:\n",
        "                keep_output = False\n",
        "                break\n",
        "\n",
        "        if keep_input and keep_output:\n",
        "            keep_pairs.append(pair)\n",
        "\n",
        "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
        "    return keep_pairs\n",
        "\n",
        "def indexesFromSentence(libra, sentence):\n",
        "    return [libra.word2index[word] for word in sentence.split(' ') if word in libra.word2index] + [EOS_token]\n",
        "\n",
        "\n",
        "def zeroPadding(l, fillvalue=PAD_token):\n",
        "    padded_list = []\n",
        "    for sequence in l:\n",
        "        padded_sequence = list(sequence) + [fillvalue] * (MAX_LENGTH - len(sequence))\n",
        "        padded_list.append(padded_sequence)\n",
        "    return padded_list\n",
        "\n",
        "def inputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar\n",
        "\n",
        "# Returns padded target sequence tensor, padding mask, and max target length\n",
        "def outputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar\n",
        "\n",
        "def batch2TrainData(voc, pair_batch):\n",
        "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
        "    input_batch, output_batch = [], []\n",
        "    for pair in pair_batch:\n",
        "        input_batch.append(pair[0])\n",
        "        output_batch.append(pair[1])\n",
        "    inp = inputVar(input_batch, voc)\n",
        "    output = outputVar(output_batch, voc)\n",
        "    return inp, output"
      ],
      "metadata": {
        "id": "4WP0wMu-yiqA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Own code implementation (By me)\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "def create_padding_mask(length):\n",
        "    seq = torch.eq(length, 0)\n",
        "    return seq.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = (1 - torch.triu(torch.ones(size, size), diagonal=1)).bool()\n",
        "    return mask\n",
        "\n",
        "def positions(sequence, size):\n",
        "    if not isinstance(sequence, int):\n",
        "        raise ValueError(f\"Invalid type for sequence. Expected int, got: {type(sequence)}\")\n",
        "\n",
        "    if not isinstance(size, int):\n",
        "        raise ValueError(f\"Invalid type for size. Expected int, got: {type(size)}\")\n",
        "\n",
        "    if sequence <= 0:\n",
        "        raise ValueError(\"Sequence length must be positive.\")\n",
        "\n",
        "    if size <= 0:\n",
        "        raise ValueError(\"Size of positional encoding must be positive.\")\n",
        "\n",
        "    batch_size = sequence\n",
        "    pos = torch.arange(batch_size).float().unsqueeze(1)\n",
        "    i = torch.arange(size).float().unsqueeze(0)\n",
        "    angles = pos / 10000 ** (2 * (i // 2) / size)\n",
        "    angles = angles.reshape(batch_size, size)\n",
        "\n",
        "    pe = torch.zeros(batch_size, size)\n",
        "    pe[:, 0::2] = torch.sin(angles[:, 0::2])\n",
        "    pe[:, 1::2] = torch.cos(angles[:, 1::2])\n",
        "\n",
        "    return pe\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    y_pred_argmax = torch.argmax(y_pred, dim=-1)\n",
        "    correct = (y_pred_argmax == y_true).float().sum()\n",
        "    total = y_true.numel()\n",
        "    return correct / total\n",
        "\n",
        "class CustomSchedule(LambdaLR):\n",
        "    def __init__(self, optimizer, d_model, warmup_steps=4000):\n",
        "        self.d_model = torch.tensor(d_model, dtype=torch.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "        super(CustomSchedule, self).__init__(optimizer, self.lr_lambda)\n",
        "\n",
        "    def lr_lambda(self, step):\n",
        "        step_float = torch.tensor(step, dtype=torch.float32)\n",
        "        arg1 = torch.rsqrt(step_float)\n",
        "        arg2 = step_float * (self.warmup_steps**-1.5)\n",
        "        return (torch.rsqrt(self.d_model) * torch.minimum(arg1, arg2))\n",
        "\n",
        "class Normalize(nn.Module):\n",
        "  def __init__(self, scale: float, shift:float,):\n",
        "    super(Normalize, self).__init__()\n",
        "    self.scale = scale\n",
        "    self.shift = shift\n",
        "\n",
        "  def forward(self, x):\n",
        "      mean = torch.mean(x)\n",
        "      deviation = torch.std(x)\n",
        "      x = (x - mean) / deviation\n",
        "      x = x * self.scale\n",
        "      x = x + self.shift\n",
        "      return x\n",
        "\n",
        "\n",
        "#---- Self Made Multi Head Attention Implementation ----\n",
        "class Multi_Head_Attention(nn.Module):\n",
        "    def __init__(self, embedding_size, head_num, batch_size, learning_rate):\n",
        "        super(Multi_Head_Attention, self).__init__()\n",
        "\n",
        "        self.embedding_size = embedding_size\n",
        "        self.num_heads = head_num\n",
        "        self.learning_rate = learning_rate\n",
        "        self.head_size = embedding_size // self.num_heads\n",
        "\n",
        "\n",
        "        self.w_q = nn.Linear(embedding_size, embedding_size*self.num_heads, bias=False)\n",
        "        self.w_k = nn.Linear(embedding_size, embedding_size*self.num_heads, bias=False)\n",
        "        self.w_v = nn.Linear(embedding_size, embedding_size*self.num_heads, bias=False)\n",
        "\n",
        "        self.w_o = nn.Linear(embedding_size*self.num_heads, embedding_size, bias=False) # dense linear pojection\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = x.reshape(batch_size, -1, self.num_heads, self.head_size)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, x, blank, mask=None):\n",
        "        distribution = torch.matmul(q, k.transpose(2,3))/ np.sqrt(self.embedding_size)\n",
        "\n",
        "        if mask is not None:\n",
        "            distribution.masked_fill(mask, float('-inf'))\n",
        "\n",
        "        distribution = F.softmax(distribution, dim=-1)\n",
        "        weight = torch.matmul(distribution, v)\n",
        "        return weight\n",
        "\n",
        "    def forward(self, input_d, blank=None, mask=None ):\n",
        "        #shape: batch / length /embedding_size\n",
        "        batch, length, size = input_d.shape\n",
        "\n",
        "        q = self.w_q(input_d)\n",
        "        k = self.w_k(input_d)\n",
        "        v = self.w_v(input_d)\n",
        "\n",
        "        q = self.split_heads(q, batch)\n",
        "        k = self.split_heads(k, batch)\n",
        "        v = self.split_heads(v, batch)\n",
        "\n",
        "        product = self.scaled_dot_product_attention(q, k, v, blank, mask)\n",
        "\n",
        "        product = product.permute(0,2,1,3)\n",
        "\n",
        "        concat = product.reshape(batch, -1, self.embedding_size*self.num_heads)\n",
        "\n",
        "        out = self.w_o(concat)\n",
        "\n",
        "        return out\n",
        "\n",
        "#----Self Made Layer Encoder Implementation----\n",
        "class Encoder_Layer(nn.Module):\n",
        "    def __init__(self, embedding_size, head_num, batch_size, dropout, learning_rate):\n",
        "        super(Encoder_Layer, self).__init__()\n",
        "\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "        self.fc1 = nn.Linear(embedding_size, embedding_size, bias=False)\n",
        "        self.fc2 = nn.Linear(embedding_size, embedding_size, bias=False)\n",
        "\n",
        "        self.attention = Multi_Head_Attention(embedding_size, head_num, batch_size, learning_rate)\n",
        "\n",
        "        self.norm = Normalize(0.4, 0.4)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_d, mask):\n",
        "        attn = self.attention(input_d, None, mask)\n",
        "        # attn shape: batch / length / embedding_size\n",
        "\n",
        "        attn_d = self.dropout(attn)\n",
        "        # each word will independently have a chance to re-initilise it's representation ( [0.1, 0.3, -0.1, 0.2] ) with all 0s\n",
        "\n",
        "        attn_dn = self.norm(input_d + attn_d)\n",
        "        # It should be normalizing, scaling and shifting\n",
        "\n",
        "        attn_lin1 = self.fc1(attn_dn)\n",
        "        # the preservation of vector addition and scalar multiplication [2,3]+[1,6] -linear-> [6,18]\n",
        "\n",
        "        attn_rel = F.relu(attn_lin1)\n",
        "        # activation function that looks at each dimensional cell and returns 0 if negative or if positive returns back the number itself\n",
        "\n",
        "\n",
        "        attn_lin2 = self.fc2(attn_rel)\n",
        "        # the preservation of vector addition and scalar multiplication [2,3]+[1,6] -linear-> [6,18]\n",
        "\n",
        "        attn_d = self.dropout(attn_lin2)\n",
        "        # each word will independently have a chance to re-initilise it's representation ( [0.1, 0.3, -0.1, 0.2] ) with all 0s\n",
        "\n",
        "        out = self.norm(attn_dn + attn_d)\n",
        "        # It should be normalizing, scaling and shifting\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "#----Self Made Encoder Implementation----\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embedding, embedding_size, head_num, batch_size, dropout, learning_rate, n_layers=2):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.num_layers = n_layers\n",
        "        self.encoder_layers = nn.ModuleList([Encoder_Layer(embedding_size, head_num, batch_size, dropout, learning_rate) for _ in range(n_layers)])\n",
        "\n",
        "        self.embedding = embedding\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # shape: batch / length\n",
        "        batch, length = x.shape\n",
        "\n",
        "        input_variable = model.embedding(x).to(device)\n",
        "        # shape: batch / length / embedding_size\n",
        "\n",
        "        pos = positions(length, self.embedding_size).to(device)\n",
        "        input_pos = pos + input_variable\n",
        "        # we add pos to x's high dimensional space as context for each word position (cannot be seen as 1 more dimension)\n",
        "        # shape: batch, length / embedding_size / --high-->  pos\n",
        "        # \"What\": [0.1, 0.3, -0.1, 0.2] + Position 1: [0.1, 0.2, -0.1, -0.2] = [0.2, 0.5, -0.2, 0.0]\n",
        "\n",
        "        input_d = self.dropout(input_pos).to(device)\n",
        "        # each word will independently have a chance to re-initilise it's representation ( [0.1, 0.3, -0.1, 0.2] ) with all 0s\n",
        "\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            output = encoder_layer(input_d, mask)\n",
        "        # for each encoder_layer generate an output quite simple.\n",
        "\n",
        "        return output # Send their output 'down the drain' (to decoder) as 1 variable  :))\n",
        "\n",
        "#----Self Made Layer Decoder Implementation----\n",
        "class Decoder_Layer(nn.Module):\n",
        "    def __init__(self, embedding_size, head_num, batch_size, dropout, learning_rate):\n",
        "        super(Decoder_Layer, self).__init__()\n",
        "\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "        self.fc1 = nn.Linear(embedding_size, embedding_size, bias=False)\n",
        "        self.fc2 = nn.Linear(embedding_size, embedding_size, bias=False)\n",
        "\n",
        "        self.attention = Multi_Head_Attention(embedding_size, head_num, batch_size, learning_rate)\n",
        "\n",
        "        self.norm = Normalize(0.4, 0.4)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, enc_o, blank, ahead, mask):\n",
        "        attn1 = self.attention(enc_o, None, mask)\n",
        "\n",
        "        attn_n = self.norm(attn1 + enc_o)\n",
        "        # It should be normalizing, scaling and shifting\n",
        "\n",
        "        attn2 = self.attention(enc_o, blank, ahead)\n",
        "\n",
        "        attn_d = self.dropout(attn2)\n",
        "        # each word will independently have a chance to re-initilise it's representation ( [0.1, 0.3, -0.1, 0.2] ) with all 0s\n",
        "\n",
        "        attn_n2d = self.norm(attn_n + attn_d)\n",
        "        # It should be normalizing, scaling and shifting\n",
        "\n",
        "        attn_lin1 = self.fc1(attn_n2d)\n",
        "        # the preservation of vector addition and scalar multiplication [2,3]+[1,6] -linear-> [6,18]\n",
        "\n",
        "        attn_rel = F.relu(attn_lin1)\n",
        "        # activation function that looks at each dimensional cell and returns 0 if negative or if positive returns back the number itself\n",
        "\n",
        "        attn_lin2 = self.fc2(attn_rel)\n",
        "        # the preservation of vector addition and scalar multiplication [2,3]+[1,6] -linear-> [6,18]\n",
        "\n",
        "        attn_d = self.dropout(attn_lin2)\n",
        "        # each word will independently have a chance to re-initilise it's representation ( [0.1, 0.3, -0.1, 0.2] ) with all 0s\n",
        "\n",
        "        output = self.norm(attn_d + attn_n2d)\n",
        "        # It should be normalizing, scaling and shifting\n",
        "\n",
        "        return output\n",
        "\n",
        "#----Self Made Decoder Implementation----\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embedding, embedding_size, head_num, batch_size, dropout, learning_rate, n_layers):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = embedding\n",
        "        self.embedding_size = embedding_size\n",
        "        self.num_layers = n_layers\n",
        "\n",
        "        self.decoder_layers = nn.ModuleList([Decoder_Layer(embedding_size, head_num, batch_size, dropout, learning_rate) for _ in range(n_layers)])\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, decoder_input, enc_output, ahead=None, mask=None):\n",
        "        batch, length, embedding_size = enc_output.shape\n",
        "        #INPUTS\n",
        "        # decoder_input shape: batch / length\n",
        "        # enc_output shape: batch / length / embedding_size\n",
        "\n",
        "        #MASKS\n",
        "        # ahead shape: length / length\n",
        "        # mask shape: batch / length / embedding_size\n",
        "\n",
        "        embedded = self.embedding(decoder_input).to(device)\n",
        "        # shape: batch / length / embedding_size\n",
        "\n",
        "\n",
        "        pos = positions(length, self.embedding_size).to(device)\n",
        "        input_pos = pos + enc_output\n",
        "        # we add pos to x's high dimensional space as context for each word position (cannot be seen as 1 more dimension)\n",
        "        # shape: batch, length / embedding_size / --high-->  pos\n",
        "        # \"What\": [0.1, 0.3, -0.1, 0.2] + Position 1: [0.1, 0.2, -0.1, -0.2] = [0.2, 0.5, -0.2, 0.0]\n",
        "\n",
        "\n",
        "        input_d = self.dropout(input_pos)\n",
        "        # each word will independently have a chance to re-initilise it's representation ( [0.1, 0.3, -0.1, 0.2] ) with all 0s\n",
        "\n",
        "        for decoder_layer in self.decoder_layers:\n",
        "            output = decoder_layer(input_d, embedded, ahead, mask)\n",
        "        # for each decoder_layer generate an output quite simple.\n",
        "\n",
        "        return output # Send their output 'down the drain' (the Transformer) as 1 variable  :))\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embedding_size, head_num, batch_size, dropout, learning_rate, decoder_learning_ratio,\n",
        "                 encoder_n_layers, decoder_n_layers, vocab_size, libra, task='train', loadFilename=None):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.embedding_size = embedding_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.batch_size = batch_size\n",
        "        self.num_heads = head_num\n",
        "        self.task = task\n",
        "\n",
        "        self.fc = nn.Linear(embedding_size, vocab_size, bias=False)\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "\n",
        "        self.decoder = Decoder(self.embedding, embedding_size, head_num, batch_size, dropout, learning_rate, encoder_n_layers)\n",
        "        self.encoder = Encoder(self.embedding, embedding_size, head_num, batch_size, dropout, learning_rate, decoder_n_layers)\n",
        "\n",
        "        if loadFilename:\n",
        "            print(\"Set to: 'trained model'\")\n",
        "            self.checkpoint = torch.load(loadFilename, map_location=device)\n",
        "            encoder_sd = self.checkpoint['en']\n",
        "            decoder_sd = self.checkpoint['de']\n",
        "            encoder_optimizer_sd = self.checkpoint['en_opt']\n",
        "            decoder_optimizer_sd = self.checkpoint['de_opt']\n",
        "            embedding_sd = self.checkpoint['embedding']\n",
        "            libra.__dict__ = self.checkpoint['voc_dict']\n",
        "            print(\"Loss: \",self.checkpoint[\"loss\"])\n",
        "            print(\"Time: \",self.checkpoint[\"time\"])\n",
        "\n",
        "        else:\n",
        "            print(\"Set to: 'new model'\")\n",
        "\n",
        "        if loadFilename:\n",
        "            self.embedding.load_state_dict(embedding_sd)\n",
        "\n",
        "        if loadFilename:\n",
        "            self.encoder.load_state_dict(encoder_sd)\n",
        "            self.decoder.load_state_dict(decoder_sd)\n",
        "\n",
        "        self.embedding = self.embedding.to(device)\n",
        "        self.encoder = self.encoder.to(device)\n",
        "        self.decoder = self.decoder.to(device)\n",
        "\n",
        "        if task == \"train\":\n",
        "            self.encoder.train()\n",
        "            self.decoder.train()\n",
        "        else:\n",
        "            self.encoder.eval()\n",
        "            self.decoder.eval()\n",
        "\n",
        "        self.decoder_optimizer = optim.Adam(self.decoder.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
        "        self.encoder_optimizer = optim.Adam(self.encoder.parameters(), lr=learning_rate * decoder_learning_ratio, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "        self.decoder_scheduler = CustomSchedule(self.decoder_optimizer, embedding_size)\n",
        "        self.encoder_scheduler = CustomSchedule(self.encoder_optimizer, embedding_size)\n",
        "\n",
        "        if loadFilename:\n",
        "            self.encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
        "            self.decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
        "\n",
        "    def forward(self, input_variable):\n",
        "        # shape:  batch / length\n",
        "        batch, length = input_variable.shape\n",
        "\n",
        "        enc_padding_mask = create_padding_mask(torch.LongTensor(length)).to(device)\n",
        "        dec_padding_mask = create_padding_mask(torch.LongTensor(length)).to(device)\n",
        "        look_ahead_mask  = create_look_ahead_mask(self.embedding_size // self.num_heads).to(device)\n",
        "\n",
        "        decoder_input = torch.LongTensor([[SOS_token for _ in range(batch)]]).to(device)\n",
        "\n",
        "        enc_output = self.encoder(input_variable, enc_padding_mask)\n",
        "        dec_output = self.decoder(decoder_input, enc_output, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        output = self.fc(dec_output)\n",
        "        decoder_output_probs = F.softmax(output, dim=-1)\n",
        "\n",
        "        return decoder_output_probs"
      ],
      "metadata": {
        "id": "FsqQgKMpY-GS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(input_variable, target_variable, vocab_size, model, clip, max_length=MAX_LENGTH):\n",
        "\n",
        "    model.encoder_optimizer.zero_grad()\n",
        "    model.decoder_optimizer.zero_grad()\n",
        "\n",
        "    t_loss = 0\n",
        "    t_accuracy = 0\n",
        "    n_totals = 0\n",
        "\n",
        "    for t in range(max_length):\n",
        "        decoder_output = model(input_variable)\n",
        "        loss = F.cross_entropy(decoder_output.view(-1, vocab_size), target_variable.reshape(-1), ignore_index=0)\n",
        "        mask = (target_variable != 0).float()\n",
        "        loss = (loss * mask).mean()\n",
        "        accuracy_v = accuracy(target_variable, decoder_output)\n",
        "        t_loss += loss\n",
        "        t_accuracy += accuracy_v.item()\n",
        "        n_totals += 1\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    model.encoder_optimizer.step()\n",
        "    model.decoder_optimizer.step()\n",
        "\n",
        "    model.encoder_scheduler.step()\n",
        "    model.decoder_scheduler.step()\n",
        "\n",
        "    _ = nn.utils.clip_grad_norm_(model.encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(model.decoder.parameters(), clip)\n",
        "\n",
        "    return t_loss / n_totals, t_accuracy / n_totals"
      ],
      "metadata": {
        "id": "FxPpFVM2zEVM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainIters(model_name, libra, pairs, save_dir, n_iteration, batch_size,\n",
        "               print_every, save_every, clip, loadFilename, vocab_size, model):\n",
        "\n",
        "    print(\"Creating the training batches...\")\n",
        "    training_pairs = [batch2TrainData(libra, [random.choice(pairs) for _ in range(batch_size)])\n",
        "                      for _ in range(n_iteration)]\n",
        "\n",
        "    start_iteration = 1\n",
        "    print_loss = 0\n",
        "    print_count = 0\n",
        "    old = 0\n",
        "    tries = 0\n",
        "\n",
        "    if loadFilename:\n",
        "        tries = model.checkpoint['time']\n",
        "\n",
        "    print(\"Initializing Training...\")\n",
        "    print()\n",
        "    for iteration in range(start_iteration, n_iteration + 1):\n",
        "        training_pair = training_pairs[iteration - 1]\n",
        "\n",
        "        input_variable, target_variable = training_pair\n",
        "        # batch / length\n",
        "\n",
        "        input_variable = input_variable.to(device)\n",
        "        target_variable = target_variable.to(device)\n",
        "\n",
        "        loss, accuracy = train(input_variable, target_variable, vocab_size, model, clip)\n",
        "\n",
        "        if iteration % print_every == 0:\n",
        "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}; Average accuracy: {:.4f}\".format(iteration, iteration / n_iteration * 100, loss, accuracy))\n",
        "\n",
        "\n",
        "        if (iteration % save_every == 0):\n",
        "                    tries += save_every\n",
        "                    directory = os.path.join(save_dir, model_name, '{}-{}_{}'.format(model.embedding_size, model.num_heads, model.vocab_size))\n",
        "                    if not os.path.exists(directory):\n",
        "                        os.makedirs(directory)\n",
        "                    torch.save({\n",
        "                        'iteration': iteration,\n",
        "                        'time': tries,\n",
        "                        'mo':model,\n",
        "                        'en': model.encoder.state_dict(),\n",
        "                        'de': model.decoder.state_dict(),\n",
        "                        'en_opt': model.encoder_optimizer.state_dict(),\n",
        "                        'de_opt': model.decoder_optimizer.state_dict(),\n",
        "                        'loss': loss,\n",
        "                        'voc_dict': libra.__dict__,\n",
        "                        'embedding': model.embedding.state_dict()\n",
        "                    }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
      ],
      "metadata": {
        "id": "cvybKce03AI1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(sentence, model, libra, embedding_size):\n",
        "    # Tokenize the sentence\n",
        "    tokenized_sentence = [SOS_token] + [libra.word2index[word] for word in sentence.split()] + [EOS_token]\n",
        "    # Convert to tensor and add batch dimension\n",
        "    sentence_tensor = torch.tensor(tokenized_sentence, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    # Initialize output tensor with start token\n",
        "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
        "\n",
        "    for i in range(MAX_LENGTH):\n",
        "        with torch.no_grad():\n",
        "            predictions = model(sentence_tensor)\n",
        "\n",
        "        # Select the last word from the seq_len dimension\n",
        "        predictions = predictions[:, -1:, :]\n",
        "        predicted_id = torch.argmax(predictions, axis=-1)\n",
        "\n",
        "        # Return the result if the predicted_id is equal to the end token\n",
        "        if predicted_id.item() == EOS_token:\n",
        "            break\n",
        "\n",
        "        # Concatenate the predicted_id to the output\n",
        "        output = torch.cat([decoder_input, predicted_id], axis=-1)\n",
        "\n",
        "    return output.squeeze(0)\n",
        "\n",
        "\n",
        "def predict(input_sentence, model, libra, embedding_size):\n",
        "    prediction = evaluate(input_sentence, model, libra, embedding_size)\n",
        "    predicted_sentence = [libra.index2word[index.item()] for index in prediction if index.item() < libra.num_words]\n",
        "    return predicted_sentence\n",
        "\n",
        "\n",
        "def evaluateInput(model, libra, embedding_size):\n",
        "    input_sentence = ''\n",
        "    while(1):\n",
        "        try:\n",
        "            input_sentence = input('User > ')\n",
        "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
        "            input_sentence = normalizeString(input_sentence)\n",
        "            output_words = predict(input_sentence, model, libra, embedding_size)\n",
        "            print('Cleopatra:', ' '.join(output_words))\n",
        "        except KeyError:\n",
        "            print(\"Error: Encountered unknown word.\")"
      ],
      "metadata": {
        "id": "l_U2XyuVzJJN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parquet_path = \"/content/drive/MyDrive/movie-corpus/movie-corpus/0000.parquet\"\n",
        "libra, pairs = loadPrepareData(parquet_path, save_dir)\n",
        "pairs = trimRareWords(libra, pairs, MIN_COUNT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWrdvnJ4yxW8",
        "outputId": "a13dd2c2-9ee5-43d3-bdc3-0d77790fa3c5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7520 71407 7520 71407\n",
            "keep_words 7520 / 71407 = 0.1053\n",
            "Trimmed from 34570 pairs to 6799, 0.1967 of total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'Cleopatra_model'\n",
        "\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 4\n",
        "embedding_size = 256\n",
        "head_num = 4\n",
        "\n",
        "if embedding_size % head_num != 0:\n",
        "    raise ValueError(\"embedding_size / head_num must result in an integer\")\n",
        "\n",
        "dropout = 0.1\n",
        "batch_size = 10\n",
        "decoder_learning_ratio = 5.00\n",
        "learning_rate = 0.0001\n",
        "vocab_size = libra.num_words\n",
        "task = \"train\"\n",
        "\n",
        "start_model = \"no\"\n",
        "loadFilename = None if start_model == \"no\" else \"/content/drive/MyDrive/data/save/Cleopatra_model/256-4_1652/2000_checkpoint.tar\"\n",
        "\n",
        "model = Transformer(embedding_size, head_num, batch_size, dropout, learning_rate, decoder_learning_ratio,\n",
        "                 encoder_n_layers, decoder_n_layers, vocab_size, libra, task, loadFilename)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "clip = 50.0\n",
        "n_iteration = 10000\n",
        "print_every = 1\n",
        "save_every = 1000\n",
        "\n",
        "if task == \"train\":\n",
        "    trainIters(model_name, libra, pairs, save_dir, n_iteration, batch_size,\n",
        "               print_every, save_every, clip, loadFilename, vocab_size, model)\n",
        "\n",
        "if task == \"test\":\n",
        "    evaluateInput(model, libra, embedding_size)"
      ],
      "metadata": {
        "id": "dIU-IR9BzVUH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db990ff5-a56b-41c1-e0f8-5ae640026327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set to: 'new model'\n",
            "Creating the training batches...\n",
            "Initializing Training...\n",
            "\n",
            "Iteration: 1; Percent complete: 0.0%; Average loss: 4.3200; Average accuracy: 0.0000\n",
            "Iteration: 2; Percent complete: 0.0%; Average loss: 2.6599; Average accuracy: 0.0000\n",
            "Iteration: 3; Percent complete: 0.0%; Average loss: 3.4810; Average accuracy: 0.0000\n",
            "Iteration: 4; Percent complete: 0.0%; Average loss: 4.4986; Average accuracy: 0.0015\n",
            "Iteration: 5; Percent complete: 0.1%; Average loss: 4.8199; Average accuracy: 0.0000\n",
            "Iteration: 6; Percent complete: 0.1%; Average loss: 3.7131; Average accuracy: 0.0000\n",
            "Iteration: 7; Percent complete: 0.1%; Average loss: 3.8024; Average accuracy: 0.0000\n",
            "Iteration: 8; Percent complete: 0.1%; Average loss: 4.3022; Average accuracy: 0.0000\n",
            "Iteration: 9; Percent complete: 0.1%; Average loss: 4.0344; Average accuracy: 0.0000\n",
            "Iteration: 10; Percent complete: 0.1%; Average loss: 2.6956; Average accuracy: 0.0000\n",
            "Iteration: 11; Percent complete: 0.1%; Average loss: 2.7491; Average accuracy: 0.0000\n",
            "Iteration: 12; Percent complete: 0.1%; Average loss: 4.4093; Average accuracy: 0.0000\n",
            "Iteration: 13; Percent complete: 0.1%; Average loss: 5.1769; Average accuracy: 0.0000\n",
            "Iteration: 14; Percent complete: 0.1%; Average loss: 2.5171; Average accuracy: 0.0000\n",
            "Iteration: 15; Percent complete: 0.1%; Average loss: 5.0163; Average accuracy: 0.0000\n",
            "Iteration: 16; Percent complete: 0.2%; Average loss: 3.9273; Average accuracy: 0.0000\n",
            "Iteration: 17; Percent complete: 0.2%; Average loss: 2.6777; Average accuracy: 0.0000\n",
            "Iteration: 18; Percent complete: 0.2%; Average loss: 5.0341; Average accuracy: 0.0000\n",
            "Iteration: 19; Percent complete: 0.2%; Average loss: 4.7306; Average accuracy: 0.0000\n",
            "Iteration: 20; Percent complete: 0.2%; Average loss: 4.3558; Average accuracy: 0.0000\n",
            "Iteration: 21; Percent complete: 0.2%; Average loss: 3.4275; Average accuracy: 0.0000\n",
            "Iteration: 22; Percent complete: 0.2%; Average loss: 3.4275; Average accuracy: 0.0000\n",
            "Iteration: 23; Percent complete: 0.2%; Average loss: 3.6774; Average accuracy: 0.0000\n",
            "Iteration: 24; Percent complete: 0.2%; Average loss: 5.7125; Average accuracy: 0.0003\n",
            "Iteration: 25; Percent complete: 0.2%; Average loss: 5.1234; Average accuracy: 0.0002\n",
            "Iteration: 26; Percent complete: 0.3%; Average loss: 3.6238; Average accuracy: 0.0000\n",
            "Iteration: 27; Percent complete: 0.3%; Average loss: 5.4090; Average accuracy: 0.0000\n",
            "Iteration: 28; Percent complete: 0.3%; Average loss: 2.4992; Average accuracy: 0.0000\n",
            "Iteration: 29; Percent complete: 0.3%; Average loss: 3.6595; Average accuracy: 0.0000\n",
            "Iteration: 30; Percent complete: 0.3%; Average loss: 3.6060; Average accuracy: 0.0000\n",
            "Iteration: 31; Percent complete: 0.3%; Average loss: 4.0344; Average accuracy: 0.0000\n",
            "Iteration: 32; Percent complete: 0.3%; Average loss: 2.9990; Average accuracy: 0.0000\n",
            "Iteration: 33; Percent complete: 0.3%; Average loss: 4.8377; Average accuracy: 0.0000\n",
            "Iteration: 34; Percent complete: 0.3%; Average loss: 2.9812; Average accuracy: 0.0000\n",
            "Iteration: 35; Percent complete: 0.4%; Average loss: 4.2308; Average accuracy: 0.0000\n",
            "Iteration: 36; Percent complete: 0.4%; Average loss: 4.1415; Average accuracy: 0.0000\n",
            "Iteration: 37; Percent complete: 0.4%; Average loss: 5.3376; Average accuracy: 0.0000\n",
            "Iteration: 38; Percent complete: 0.4%; Average loss: 4.6949; Average accuracy: 0.0000\n",
            "Iteration: 39; Percent complete: 0.4%; Average loss: 3.3918; Average accuracy: 0.0000\n",
            "Iteration: 40; Percent complete: 0.4%; Average loss: 4.7663; Average accuracy: 0.0000\n",
            "Iteration: 41; Percent complete: 0.4%; Average loss: 3.2490; Average accuracy: 0.0000\n",
            "Iteration: 42; Percent complete: 0.4%; Average loss: 3.5346; Average accuracy: 0.0000\n",
            "Iteration: 43; Percent complete: 0.4%; Average loss: 6.2659; Average accuracy: 0.0000\n",
            "Iteration: 44; Percent complete: 0.4%; Average loss: 3.2490; Average accuracy: 0.0000\n",
            "Iteration: 45; Percent complete: 0.4%; Average loss: 2.9990; Average accuracy: 0.0000\n",
            "Iteration: 46; Percent complete: 0.5%; Average loss: 5.3376; Average accuracy: 0.0000\n",
            "Iteration: 47; Percent complete: 0.5%; Average loss: 3.6060; Average accuracy: 0.0000\n",
            "Iteration: 48; Percent complete: 0.5%; Average loss: 4.3736; Average accuracy: 0.0000\n",
            "Iteration: 49; Percent complete: 0.5%; Average loss: 3.4632; Average accuracy: 0.0000\n",
            "Iteration: 50; Percent complete: 0.5%; Average loss: 4.0344; Average accuracy: 0.0000\n",
            "Iteration: 51; Percent complete: 0.5%; Average loss: 4.8377; Average accuracy: 0.0000\n",
            "Iteration: 52; Percent complete: 0.5%; Average loss: 4.3558; Average accuracy: 0.0000\n",
            "Iteration: 53; Percent complete: 0.5%; Average loss: 4.4629; Average accuracy: 0.0000\n",
            "Iteration: 54; Percent complete: 0.5%; Average loss: 4.7485; Average accuracy: 0.0000\n",
            "Iteration: 55; Percent complete: 0.5%; Average loss: 4.2129; Average accuracy: 0.0000\n",
            "Iteration: 56; Percent complete: 0.6%; Average loss: 4.2665; Average accuracy: 0.0000\n",
            "Iteration: 57; Percent complete: 0.6%; Average loss: 5.0520; Average accuracy: 0.0000\n",
            "Iteration: 58; Percent complete: 0.6%; Average loss: 5.0520; Average accuracy: 0.0000\n",
            "Iteration: 59; Percent complete: 0.6%; Average loss: 2.4456; Average accuracy: 0.0000\n",
            "Iteration: 60; Percent complete: 0.6%; Average loss: 4.1058; Average accuracy: 0.0000\n",
            "Iteration: 61; Percent complete: 0.6%; Average loss: 4.2665; Average accuracy: 0.0000\n",
            "Iteration: 62; Percent complete: 0.6%; Average loss: 3.0704; Average accuracy: 0.0000\n",
            "Iteration: 63; Percent complete: 0.6%; Average loss: 5.3019; Average accuracy: 0.0000\n",
            "Iteration: 64; Percent complete: 0.6%; Average loss: 3.0347; Average accuracy: 0.0000\n",
            "Iteration: 65; Percent complete: 0.7%; Average loss: 5.3376; Average accuracy: 0.0000\n",
            "Iteration: 66; Percent complete: 0.7%; Average loss: 4.0344; Average accuracy: 0.0000\n",
            "Iteration: 67; Percent complete: 0.7%; Average loss: 4.5521; Average accuracy: 0.0000\n",
            "Iteration: 68; Percent complete: 0.7%; Average loss: 4.5164; Average accuracy: 0.0000\n",
            "Iteration: 69; Percent complete: 0.7%; Average loss: 4.7663; Average accuracy: 0.0000\n",
            "Iteration: 70; Percent complete: 0.7%; Average loss: 3.1776; Average accuracy: 0.0000\n",
            "Iteration: 71; Percent complete: 0.7%; Average loss: 5.2483; Average accuracy: 0.0000\n",
            "Iteration: 72; Percent complete: 0.7%; Average loss: 4.8199; Average accuracy: 0.0000\n",
            "Iteration: 73; Percent complete: 0.7%; Average loss: 5.7482; Average accuracy: 0.0000\n",
            "Iteration: 74; Percent complete: 0.7%; Average loss: 4.9270; Average accuracy: 0.0000\n",
            "Iteration: 75; Percent complete: 0.8%; Average loss: 4.8734; Average accuracy: 0.0000\n",
            "Iteration: 76; Percent complete: 0.8%; Average loss: 3.4453; Average accuracy: 0.0000\n",
            "Iteration: 77; Percent complete: 0.8%; Average loss: 4.1772; Average accuracy: 0.0000\n",
            "Iteration: 78; Percent complete: 0.8%; Average loss: 3.6774; Average accuracy: 0.0000\n",
            "Iteration: 79; Percent complete: 0.8%; Average loss: 4.4272; Average accuracy: 0.0000\n",
            "Iteration: 80; Percent complete: 0.8%; Average loss: 4.8913; Average accuracy: 0.0000\n",
            "Iteration: 81; Percent complete: 0.8%; Average loss: 3.6774; Average accuracy: 0.0000\n",
            "Iteration: 82; Percent complete: 0.8%; Average loss: 4.5521; Average accuracy: 0.0000\n",
            "Iteration: 83; Percent complete: 0.8%; Average loss: 4.8913; Average accuracy: 0.0000\n",
            "Iteration: 84; Percent complete: 0.8%; Average loss: 4.9091; Average accuracy: 0.0000\n",
            "Iteration: 85; Percent complete: 0.9%; Average loss: 4.1237; Average accuracy: 0.0000\n",
            "Iteration: 86; Percent complete: 0.9%; Average loss: 5.5696; Average accuracy: 0.0000\n",
            "Iteration: 87; Percent complete: 0.9%; Average loss: 1.9101; Average accuracy: 0.0000\n",
            "Iteration: 88; Percent complete: 0.9%; Average loss: 4.6592; Average accuracy: 0.0000\n",
            "Iteration: 89; Percent complete: 0.9%; Average loss: 4.0344; Average accuracy: 0.0000\n",
            "Iteration: 90; Percent complete: 0.9%; Average loss: 3.7488; Average accuracy: 0.0000\n",
            "Iteration: 91; Percent complete: 0.9%; Average loss: 5.2305; Average accuracy: 0.0000\n",
            "Iteration: 92; Percent complete: 0.9%; Average loss: 3.7131; Average accuracy: 0.0000\n",
            "Iteration: 93; Percent complete: 0.9%; Average loss: 3.1062; Average accuracy: 0.0000\n",
            "Iteration: 94; Percent complete: 0.9%; Average loss: 3.6595; Average accuracy: 0.0000\n",
            "Iteration: 95; Percent complete: 0.9%; Average loss: 4.1772; Average accuracy: 0.0000\n",
            "Iteration: 96; Percent complete: 1.0%; Average loss: 3.3561; Average accuracy: 0.0000\n",
            "Iteration: 97; Percent complete: 1.0%; Average loss: 3.3561; Average accuracy: 0.0000\n",
            "Iteration: 98; Percent complete: 1.0%; Average loss: 2.5528; Average accuracy: 0.0001\n",
            "Iteration: 99; Percent complete: 1.0%; Average loss: 2.4456; Average accuracy: 0.0000\n",
            "Iteration: 100; Percent complete: 1.0%; Average loss: 3.5346; Average accuracy: 0.0000\n",
            "Iteration: 101; Percent complete: 1.0%; Average loss: 4.4272; Average accuracy: 0.0000\n",
            "Iteration: 102; Percent complete: 1.0%; Average loss: 3.9452; Average accuracy: 0.0000\n",
            "Iteration: 103; Percent complete: 1.0%; Average loss: 3.3025; Average accuracy: 0.0000\n",
            "Iteration: 104; Percent complete: 1.0%; Average loss: 3.2311; Average accuracy: 0.0000\n",
            "Iteration: 105; Percent complete: 1.1%; Average loss: 4.6414; Average accuracy: 0.0000\n",
            "Iteration: 106; Percent complete: 1.1%; Average loss: 4.0523; Average accuracy: 0.0000\n",
            "Iteration: 107; Percent complete: 1.1%; Average loss: 4.2843; Average accuracy: 0.0000\n",
            "Iteration: 108; Percent complete: 1.1%; Average loss: 3.2311; Average accuracy: 0.0000\n",
            "Iteration: 109; Percent complete: 1.1%; Average loss: 4.9091; Average accuracy: 0.0000\n",
            "Iteration: 110; Percent complete: 1.1%; Average loss: 3.7667; Average accuracy: 0.0000\n",
            "Iteration: 111; Percent complete: 1.1%; Average loss: 3.7845; Average accuracy: 0.0000\n",
            "Iteration: 112; Percent complete: 1.1%; Average loss: 4.0344; Average accuracy: 0.0000\n",
            "Iteration: 113; Percent complete: 1.1%; Average loss: 4.1237; Average accuracy: 0.0000\n",
            "Iteration: 114; Percent complete: 1.1%; Average loss: 3.6774; Average accuracy: 0.0000\n",
            "Iteration: 115; Percent complete: 1.1%; Average loss: 3.4989; Average accuracy: 0.0000\n",
            "Iteration: 116; Percent complete: 1.2%; Average loss: 4.4093; Average accuracy: 0.0000\n",
            "Iteration: 117; Percent complete: 1.2%; Average loss: 2.2136; Average accuracy: 0.0000\n",
            "Iteration: 118; Percent complete: 1.2%; Average loss: 2.8741; Average accuracy: 0.0000\n",
            "Iteration: 119; Percent complete: 1.2%; Average loss: 4.1237; Average accuracy: 0.0000\n",
            "Iteration: 120; Percent complete: 1.2%; Average loss: 3.4453; Average accuracy: 0.0000\n",
            "Iteration: 121; Percent complete: 1.2%; Average loss: 3.8381; Average accuracy: 0.0000\n",
            "Iteration: 122; Percent complete: 1.2%; Average loss: 3.5703; Average accuracy: 0.0000\n",
            "Iteration: 123; Percent complete: 1.2%; Average loss: 3.5703; Average accuracy: 0.0000\n",
            "Iteration: 124; Percent complete: 1.2%; Average loss: 3.9452; Average accuracy: 0.0000\n",
            "Iteration: 125; Percent complete: 1.2%; Average loss: 4.4807; Average accuracy: 0.0000\n",
            "Iteration: 126; Percent complete: 1.3%; Average loss: 3.7845; Average accuracy: 0.0000\n",
            "Iteration: 127; Percent complete: 1.3%; Average loss: 3.7310; Average accuracy: 0.0000\n",
            "Iteration: 128; Percent complete: 1.3%; Average loss: 5.1412; Average accuracy: 0.0016\n",
            "Iteration: 129; Percent complete: 1.3%; Average loss: 5.2483; Average accuracy: 0.0000\n",
            "Iteration: 130; Percent complete: 1.3%; Average loss: 3.3739; Average accuracy: 0.0000\n",
            "Iteration: 131; Percent complete: 1.3%; Average loss: 4.6057; Average accuracy: 0.0000\n",
            "Iteration: 132; Percent complete: 1.3%; Average loss: 3.8738; Average accuracy: 0.0000\n",
            "Iteration: 133; Percent complete: 1.3%; Average loss: 4.8734; Average accuracy: 0.0000\n",
            "Iteration: 134; Percent complete: 1.3%; Average loss: 5.6768; Average accuracy: 0.0000\n",
            "Iteration: 135; Percent complete: 1.4%; Average loss: 4.1415; Average accuracy: 0.0000\n",
            "Iteration: 136; Percent complete: 1.4%; Average loss: 6.0873; Average accuracy: 0.0002\n",
            "Iteration: 137; Percent complete: 1.4%; Average loss: 4.1058; Average accuracy: 0.0000\n",
            "Iteration: 138; Percent complete: 1.4%; Average loss: 3.1954; Average accuracy: 0.0000\n",
            "Iteration: 139; Percent complete: 1.4%; Average loss: 3.5881; Average accuracy: 0.0000\n",
            "Iteration: 140; Percent complete: 1.4%; Average loss: 4.0523; Average accuracy: 0.0000\n",
            "Iteration: 141; Percent complete: 1.4%; Average loss: 3.2847; Average accuracy: 0.0000\n",
            "Iteration: 142; Percent complete: 1.4%; Average loss: 2.4099; Average accuracy: 0.0000\n",
            "Iteration: 143; Percent complete: 1.4%; Average loss: 4.9270; Average accuracy: 0.0001\n",
            "Iteration: 144; Percent complete: 1.4%; Average loss: 5.4090; Average accuracy: 0.0000\n",
            "Iteration: 145; Percent complete: 1.5%; Average loss: 3.6060; Average accuracy: 0.0000\n",
            "Iteration: 146; Percent complete: 1.5%; Average loss: 4.4450; Average accuracy: 0.0000\n",
            "Iteration: 147; Percent complete: 1.5%; Average loss: 3.6952; Average accuracy: 0.0003\n",
            "Iteration: 148; Percent complete: 1.5%; Average loss: 5.3019; Average accuracy: 0.0000\n",
            "Iteration: 149; Percent complete: 1.5%; Average loss: 3.8916; Average accuracy: 0.0000\n",
            "Iteration: 150; Percent complete: 1.5%; Average loss: 4.6592; Average accuracy: 0.0000\n",
            "Iteration: 151; Percent complete: 1.5%; Average loss: 3.6595; Average accuracy: 0.0000\n",
            "Iteration: 152; Percent complete: 1.5%; Average loss: 4.8913; Average accuracy: 0.0000\n",
            "Iteration: 153; Percent complete: 1.5%; Average loss: 3.2847; Average accuracy: 0.0000\n",
            "Iteration: 154; Percent complete: 1.5%; Average loss: 4.2129; Average accuracy: 0.0000\n",
            "Iteration: 155; Percent complete: 1.6%; Average loss: 2.8741; Average accuracy: 0.0000\n",
            "Iteration: 156; Percent complete: 1.6%; Average loss: 2.9633; Average accuracy: 0.0000\n",
            "Iteration: 157; Percent complete: 1.6%; Average loss: 2.9276; Average accuracy: 0.0000\n",
            "Iteration: 158; Percent complete: 1.6%; Average loss: 4.6414; Average accuracy: 0.0000\n",
            "Iteration: 159; Percent complete: 1.6%; Average loss: 3.7131; Average accuracy: 0.0000\n",
            "Iteration: 160; Percent complete: 1.6%; Average loss: 5.3733; Average accuracy: 0.0000\n",
            "Iteration: 161; Percent complete: 1.6%; Average loss: 4.9270; Average accuracy: 0.0000\n",
            "Iteration: 162; Percent complete: 1.6%; Average loss: 4.4450; Average accuracy: 0.0000\n",
            "Iteration: 163; Percent complete: 1.6%; Average loss: 4.8199; Average accuracy: 0.0000\n",
            "Iteration: 164; Percent complete: 1.6%; Average loss: 4.5164; Average accuracy: 0.0000\n",
            "Iteration: 165; Percent complete: 1.7%; Average loss: 3.4453; Average accuracy: 0.0000\n",
            "Iteration: 166; Percent complete: 1.7%; Average loss: 3.3025; Average accuracy: 0.0000\n",
            "Iteration: 167; Percent complete: 1.7%; Average loss: 4.8734; Average accuracy: 0.0000\n",
            "Iteration: 168; Percent complete: 1.7%; Average loss: 3.3918; Average accuracy: 0.0000\n",
            "Iteration: 169; Percent complete: 1.7%; Average loss: 4.2308; Average accuracy: 0.0000\n",
            "Iteration: 170; Percent complete: 1.7%; Average loss: 4.5521; Average accuracy: 0.0004\n",
            "Iteration: 171; Percent complete: 1.7%; Average loss: 4.3915; Average accuracy: 0.0000\n",
            "Iteration: 172; Percent complete: 1.7%; Average loss: 4.6771; Average accuracy: 0.0000\n",
            "Iteration: 173; Percent complete: 1.7%; Average loss: 4.9627; Average accuracy: 0.0000\n",
            "Iteration: 174; Percent complete: 1.7%; Average loss: 5.0877; Average accuracy: 0.0000\n",
            "Iteration: 175; Percent complete: 1.8%; Average loss: 5.2305; Average accuracy: 0.0000\n",
            "Iteration: 176; Percent complete: 1.8%; Average loss: 3.7488; Average accuracy: 0.0000\n",
            "Iteration: 177; Percent complete: 1.8%; Average loss: 2.6063; Average accuracy: 0.0000\n",
            "Iteration: 178; Percent complete: 1.8%; Average loss: 3.0169; Average accuracy: 0.0000\n",
            "Iteration: 179; Percent complete: 1.8%; Average loss: 4.6949; Average accuracy: 0.0000\n",
            "Iteration: 180; Percent complete: 1.8%; Average loss: 5.0341; Average accuracy: 0.0000\n",
            "Iteration: 181; Percent complete: 1.8%; Average loss: 2.4278; Average accuracy: 0.0000\n",
            "Iteration: 182; Percent complete: 1.8%; Average loss: 4.4629; Average accuracy: 0.0000\n",
            "Iteration: 183; Percent complete: 1.8%; Average loss: 3.7131; Average accuracy: 0.0000\n",
            "Iteration: 184; Percent complete: 1.8%; Average loss: 3.1240; Average accuracy: 0.0000\n",
            "Iteration: 185; Percent complete: 1.8%; Average loss: 3.9273; Average accuracy: 0.0000\n",
            "Iteration: 186; Percent complete: 1.9%; Average loss: 3.0704; Average accuracy: 0.0000\n",
            "Iteration: 187; Percent complete: 1.9%; Average loss: 4.5521; Average accuracy: 0.0000\n",
            "Iteration: 188; Percent complete: 1.9%; Average loss: 2.7313; Average accuracy: 0.0000\n",
            "Iteration: 189; Percent complete: 1.9%; Average loss: 3.6060; Average accuracy: 0.0000\n",
            "Iteration: 190; Percent complete: 1.9%; Average loss: 3.4096; Average accuracy: 0.0000\n",
            "Iteration: 191; Percent complete: 1.9%; Average loss: 4.5521; Average accuracy: 0.0000\n",
            "Iteration: 192; Percent complete: 1.9%; Average loss: 5.1769; Average accuracy: 0.0000\n",
            "Iteration: 193; Percent complete: 1.9%; Average loss: 2.7491; Average accuracy: 0.0000\n",
            "Iteration: 194; Percent complete: 1.9%; Average loss: 3.5703; Average accuracy: 0.0000\n",
            "Iteration: 195; Percent complete: 1.9%; Average loss: 4.4629; Average accuracy: 0.0000\n",
            "Iteration: 196; Percent complete: 2.0%; Average loss: 4.5700; Average accuracy: 0.0000\n",
            "Iteration: 197; Percent complete: 2.0%; Average loss: 4.9627; Average accuracy: 0.0000\n",
            "Iteration: 198; Percent complete: 2.0%; Average loss: 4.5700; Average accuracy: 0.0000\n",
            "Iteration: 199; Percent complete: 2.0%; Average loss: 5.3376; Average accuracy: 0.0000\n",
            "Iteration: 200; Percent complete: 2.0%; Average loss: 3.1954; Average accuracy: 0.0000\n",
            "Iteration: 201; Percent complete: 2.0%; Average loss: 4.5521; Average accuracy: 0.0000\n",
            "Iteration: 202; Percent complete: 2.0%; Average loss: 3.7845; Average accuracy: 0.0000\n",
            "Iteration: 203; Percent complete: 2.0%; Average loss: 3.1954; Average accuracy: 0.0000\n",
            "Iteration: 204; Percent complete: 2.0%; Average loss: 4.8020; Average accuracy: 0.0000\n",
            "Iteration: 205; Percent complete: 2.1%; Average loss: 5.9624; Average accuracy: 0.0000\n",
            "Iteration: 206; Percent complete: 2.1%; Average loss: 4.9806; Average accuracy: 0.0000\n",
            "Iteration: 207; Percent complete: 2.1%; Average loss: 4.4986; Average accuracy: 0.0000\n",
            "Iteration: 208; Percent complete: 2.1%; Average loss: 3.9452; Average accuracy: 0.0000\n",
            "Iteration: 209; Percent complete: 2.1%; Average loss: 4.4629; Average accuracy: 0.0000\n",
            "Iteration: 210; Percent complete: 2.1%; Average loss: 3.4275; Average accuracy: 0.0000\n",
            "Iteration: 211; Percent complete: 2.1%; Average loss: 4.3379; Average accuracy: 0.0000\n",
            "Iteration: 212; Percent complete: 2.1%; Average loss: 3.8024; Average accuracy: 0.0000\n",
            "Iteration: 213; Percent complete: 2.1%; Average loss: 3.3918; Average accuracy: 0.0000\n",
            "Iteration: 214; Percent complete: 2.1%; Average loss: 5.2126; Average accuracy: 0.0000\n",
            "Iteration: 215; Percent complete: 2.1%; Average loss: 4.1237; Average accuracy: 0.0000\n",
            "Iteration: 216; Percent complete: 2.2%; Average loss: 2.7313; Average accuracy: 0.0000\n",
            "Iteration: 217; Percent complete: 2.2%; Average loss: 3.9987; Average accuracy: 0.0000\n",
            "Iteration: 218; Percent complete: 2.2%; Average loss: 4.1415; Average accuracy: 0.0000\n",
            "Iteration: 219; Percent complete: 2.2%; Average loss: 3.0883; Average accuracy: 0.0002\n",
            "Iteration: 220; Percent complete: 2.2%; Average loss: 4.4272; Average accuracy: 0.0000\n",
            "Iteration: 221; Percent complete: 2.2%; Average loss: 4.8734; Average accuracy: 0.0000\n",
            "Iteration: 222; Percent complete: 2.2%; Average loss: 4.3200; Average accuracy: 0.0000\n",
            "Iteration: 223; Percent complete: 2.2%; Average loss: 5.4982; Average accuracy: 0.0000\n",
            "Iteration: 224; Percent complete: 2.2%; Average loss: 4.7128; Average accuracy: 0.0000\n",
            "Iteration: 225; Percent complete: 2.2%; Average loss: 3.3382; Average accuracy: 0.0000\n",
            "Iteration: 226; Percent complete: 2.3%; Average loss: 3.2668; Average accuracy: 0.0000\n",
            "Iteration: 227; Percent complete: 2.3%; Average loss: 4.2486; Average accuracy: 0.0000\n",
            "Iteration: 228; Percent complete: 2.3%; Average loss: 1.3746; Average accuracy: 0.0000\n",
            "Iteration: 229; Percent complete: 2.3%; Average loss: 3.6060; Average accuracy: 0.0000\n",
            "Iteration: 230; Percent complete: 2.3%; Average loss: 4.5343; Average accuracy: 0.0000\n",
            "Iteration: 231; Percent complete: 2.3%; Average loss: 3.4989; Average accuracy: 0.0000\n",
            "Iteration: 232; Percent complete: 2.3%; Average loss: 4.4272; Average accuracy: 0.0000\n",
            "Iteration: 233; Percent complete: 2.3%; Average loss: 4.3022; Average accuracy: 0.0000\n",
            "Iteration: 234; Percent complete: 2.3%; Average loss: 4.7128; Average accuracy: 0.0000\n",
            "Iteration: 235; Percent complete: 2.4%; Average loss: 3.9809; Average accuracy: 0.0000\n",
            "Iteration: 236; Percent complete: 2.4%; Average loss: 3.6774; Average accuracy: 0.0000\n",
            "Iteration: 237; Percent complete: 2.4%; Average loss: 5.3554; Average accuracy: 0.0000\n",
            "Iteration: 238; Percent complete: 2.4%; Average loss: 5.2126; Average accuracy: 0.0000\n",
            "Iteration: 239; Percent complete: 2.4%; Average loss: 3.1776; Average accuracy: 0.0000\n",
            "Iteration: 240; Percent complete: 2.4%; Average loss: 4.1415; Average accuracy: 0.0000\n",
            "Iteration: 241; Percent complete: 2.4%; Average loss: 3.2847; Average accuracy: 0.0000\n",
            "Iteration: 242; Percent complete: 2.4%; Average loss: 4.9270; Average accuracy: 0.0000\n",
            "Iteration: 243; Percent complete: 2.4%; Average loss: 4.1237; Average accuracy: 0.0000\n",
            "Iteration: 244; Percent complete: 2.4%; Average loss: 4.6414; Average accuracy: 0.0000\n",
            "Iteration: 245; Percent complete: 2.5%; Average loss: 2.8384; Average accuracy: 0.0000\n",
            "Iteration: 246; Percent complete: 2.5%; Average loss: 3.9273; Average accuracy: 0.0000\n",
            "Iteration: 247; Percent complete: 2.5%; Average loss: 4.0701; Average accuracy: 0.0000\n",
            "Iteration: 248; Percent complete: 2.5%; Average loss: 3.7667; Average accuracy: 0.0000\n",
            "Iteration: 249; Percent complete: 2.5%; Average loss: 3.7845; Average accuracy: 0.0000\n",
            "Iteration: 250; Percent complete: 2.5%; Average loss: 4.3915; Average accuracy: 0.0001\n",
            "Iteration: 251; Percent complete: 2.5%; Average loss: 4.4629; Average accuracy: 0.0000\n",
            "Iteration: 252; Percent complete: 2.5%; Average loss: 3.5167; Average accuracy: 0.0002\n",
            "Iteration: 253; Percent complete: 2.5%; Average loss: 3.4989; Average accuracy: 0.0000\n",
            "Iteration: 254; Percent complete: 2.5%; Average loss: 4.6592; Average accuracy: 0.0000\n",
            "Iteration: 255; Percent complete: 2.5%; Average loss: 2.5349; Average accuracy: 0.0000\n",
            "Iteration: 256; Percent complete: 2.6%; Average loss: 5.2483; Average accuracy: 0.0000\n",
            "Iteration: 257; Percent complete: 2.6%; Average loss: 5.1948; Average accuracy: 0.0000\n",
            "Iteration: 258; Percent complete: 2.6%; Average loss: 3.5703; Average accuracy: 0.0000\n",
            "Iteration: 259; Percent complete: 2.6%; Average loss: 5.8910; Average accuracy: 0.0000\n",
            "Iteration: 260; Percent complete: 2.6%; Average loss: 4.1594; Average accuracy: 0.0000\n",
            "Iteration: 261; Percent complete: 2.6%; Average loss: 3.9095; Average accuracy: 0.0000\n",
            "Iteration: 262; Percent complete: 2.6%; Average loss: 2.9990; Average accuracy: 0.0000\n",
            "Iteration: 263; Percent complete: 2.6%; Average loss: 4.4629; Average accuracy: 0.0000\n",
            "Iteration: 264; Percent complete: 2.6%; Average loss: 4.2308; Average accuracy: 0.0000\n",
            "Iteration: 265; Percent complete: 2.6%; Average loss: 4.6592; Average accuracy: 0.0000\n",
            "Iteration: 266; Percent complete: 2.7%; Average loss: 3.4989; Average accuracy: 0.0000\n",
            "Iteration: 267; Percent complete: 2.7%; Average loss: 3.3918; Average accuracy: 0.0000\n",
            "Iteration: 268; Percent complete: 2.7%; Average loss: 4.3558; Average accuracy: 0.0000\n",
            "Iteration: 269; Percent complete: 2.7%; Average loss: 3.3025; Average accuracy: 0.0000\n",
            "Iteration: 270; Percent complete: 2.7%; Average loss: 5.4090; Average accuracy: 0.0000\n",
            "Iteration: 271; Percent complete: 2.7%; Average loss: 5.1412; Average accuracy: 0.0000\n",
            "Iteration: 272; Percent complete: 2.7%; Average loss: 4.6949; Average accuracy: 0.0000\n",
            "Iteration: 273; Percent complete: 2.7%; Average loss: 4.8734; Average accuracy: 0.0000\n",
            "Iteration: 274; Percent complete: 2.7%; Average loss: 4.2843; Average accuracy: 0.0000\n",
            "Iteration: 275; Percent complete: 2.8%; Average loss: 3.3025; Average accuracy: 0.0000\n",
            "Iteration: 276; Percent complete: 2.8%; Average loss: 4.9449; Average accuracy: 0.0000\n",
            "Iteration: 277; Percent complete: 2.8%; Average loss: 3.4810; Average accuracy: 0.0000\n",
            "Iteration: 278; Percent complete: 2.8%; Average loss: 4.4272; Average accuracy: 0.0000\n",
            "Iteration: 279; Percent complete: 2.8%; Average loss: 4.0880; Average accuracy: 0.0000\n",
            "Iteration: 280; Percent complete: 2.8%; Average loss: 3.2490; Average accuracy: 0.0000\n",
            "Iteration: 281; Percent complete: 2.8%; Average loss: 4.5878; Average accuracy: 0.0000\n",
            "Iteration: 282; Percent complete: 2.8%; Average loss: 4.6057; Average accuracy: 0.0000\n",
            "Iteration: 283; Percent complete: 2.8%; Average loss: 4.1058; Average accuracy: 0.0000\n",
            "Iteration: 284; Percent complete: 2.8%; Average loss: 4.8020; Average accuracy: 0.0000\n",
            "Iteration: 285; Percent complete: 2.9%; Average loss: 3.1954; Average accuracy: 0.0000\n",
            "Iteration: 286; Percent complete: 2.9%; Average loss: 3.6060; Average accuracy: 0.0000\n",
            "Iteration: 287; Percent complete: 2.9%; Average loss: 4.4093; Average accuracy: 0.0000\n",
            "Iteration: 288; Percent complete: 2.9%; Average loss: 3.9273; Average accuracy: 0.0000\n",
            "Iteration: 289; Percent complete: 2.9%; Average loss: 4.7128; Average accuracy: 0.0000\n",
            "Iteration: 290; Percent complete: 2.9%; Average loss: 4.1058; Average accuracy: 0.0000\n",
            "Iteration: 291; Percent complete: 2.9%; Average loss: 2.5528; Average accuracy: 0.0000\n",
            "Iteration: 292; Percent complete: 2.9%; Average loss: 3.8916; Average accuracy: 0.0000\n",
            "Iteration: 293; Percent complete: 2.9%; Average loss: 3.1240; Average accuracy: 0.0000\n",
            "Iteration: 294; Percent complete: 2.9%; Average loss: 3.1240; Average accuracy: 0.0000\n",
            "Iteration: 295; Percent complete: 2.9%; Average loss: 4.2665; Average accuracy: 0.0000\n",
            "Iteration: 296; Percent complete: 3.0%; Average loss: 4.3736; Average accuracy: 0.0000\n",
            "Iteration: 297; Percent complete: 3.0%; Average loss: 5.3911; Average accuracy: 0.0000\n",
            "Iteration: 298; Percent complete: 3.0%; Average loss: 3.2847; Average accuracy: 0.0000\n",
            "Iteration: 299; Percent complete: 3.0%; Average loss: 4.2486; Average accuracy: 0.0000\n",
            "Iteration: 300; Percent complete: 3.0%; Average loss: 5.1412; Average accuracy: 0.0000\n",
            "Iteration: 301; Percent complete: 3.0%; Average loss: 3.8202; Average accuracy: 0.0000\n",
            "Iteration: 302; Percent complete: 3.0%; Average loss: 4.4272; Average accuracy: 0.0000\n",
            "Iteration: 303; Percent complete: 3.0%; Average loss: 5.4982; Average accuracy: 0.0001\n",
            "Iteration: 304; Percent complete: 3.0%; Average loss: 3.0704; Average accuracy: 0.0000\n",
            "Iteration: 305; Percent complete: 3.0%; Average loss: 4.6414; Average accuracy: 0.0000\n",
            "Iteration: 306; Percent complete: 3.1%; Average loss: 3.1776; Average accuracy: 0.0000\n",
            "Iteration: 307; Percent complete: 3.1%; Average loss: 4.2308; Average accuracy: 0.0000\n",
            "Iteration: 308; Percent complete: 3.1%; Average loss: 2.9812; Average accuracy: 0.0000\n",
            "Iteration: 309; Percent complete: 3.1%; Average loss: 6.1944; Average accuracy: 0.0000\n",
            "Iteration: 310; Percent complete: 3.1%; Average loss: 2.9098; Average accuracy: 0.0000\n",
            "Iteration: 311; Percent complete: 3.1%; Average loss: 4.1772; Average accuracy: 0.0000\n",
            "Iteration: 312; Percent complete: 3.1%; Average loss: 4.6057; Average accuracy: 0.0000\n",
            "Iteration: 313; Percent complete: 3.1%; Average loss: 4.0523; Average accuracy: 0.0000\n",
            "Iteration: 314; Percent complete: 3.1%; Average loss: 4.3736; Average accuracy: 0.0000\n",
            "Iteration: 315; Percent complete: 3.1%; Average loss: 4.7306; Average accuracy: 0.0015\n",
            "Iteration: 316; Percent complete: 3.2%; Average loss: 3.6774; Average accuracy: 0.0000\n",
            "Iteration: 317; Percent complete: 3.2%; Average loss: 3.7310; Average accuracy: 0.0000\n",
            "Iteration: 318; Percent complete: 3.2%; Average loss: 3.2311; Average accuracy: 0.0000\n",
            "Iteration: 319; Percent complete: 3.2%; Average loss: 4.6414; Average accuracy: 0.0000\n",
            "Iteration: 320; Percent complete: 3.2%; Average loss: 2.8919; Average accuracy: 0.0000\n",
            "Iteration: 321; Percent complete: 3.2%; Average loss: 4.4986; Average accuracy: 0.0000\n",
            "Iteration: 322; Percent complete: 3.2%; Average loss: 4.5521; Average accuracy: 0.0000\n",
            "Iteration: 323; Percent complete: 3.2%; Average loss: 4.6592; Average accuracy: 0.0000\n",
            "Iteration: 324; Percent complete: 3.2%; Average loss: 2.3742; Average accuracy: 0.0000\n",
            "Iteration: 325; Percent complete: 3.2%; Average loss: 4.4450; Average accuracy: 0.0000\n",
            "Iteration: 326; Percent complete: 3.3%; Average loss: 4.8734; Average accuracy: 0.0000\n",
            "Iteration: 327; Percent complete: 3.3%; Average loss: 3.4989; Average accuracy: 0.0000\n",
            "Iteration: 328; Percent complete: 3.3%; Average loss: 3.9809; Average accuracy: 0.0000\n",
            "Iteration: 329; Percent complete: 3.3%; Average loss: 2.9990; Average accuracy: 0.0000\n",
            "Iteration: 330; Percent complete: 3.3%; Average loss: 4.4450; Average accuracy: 0.0000\n",
            "Iteration: 331; Percent complete: 3.3%; Average loss: 3.5346; Average accuracy: 0.0000\n",
            "Iteration: 332; Percent complete: 3.3%; Average loss: 4.4093; Average accuracy: 0.0000\n",
            "Iteration: 333; Percent complete: 3.3%; Average loss: 3.7488; Average accuracy: 0.0000\n",
            "Iteration: 334; Percent complete: 3.3%; Average loss: 4.1951; Average accuracy: 0.0000\n",
            "Iteration: 335; Percent complete: 3.4%; Average loss: 5.5161; Average accuracy: 0.0000\n",
            "Iteration: 336; Percent complete: 3.4%; Average loss: 3.6238; Average accuracy: 0.0000\n",
            "Iteration: 337; Percent complete: 3.4%; Average loss: 3.7845; Average accuracy: 0.0000\n",
            "Iteration: 338; Percent complete: 3.4%; Average loss: 2.6956; Average accuracy: 0.0000\n",
            "Iteration: 339; Percent complete: 3.4%; Average loss: 3.9095; Average accuracy: 0.0000\n",
            "Iteration: 340; Percent complete: 3.4%; Average loss: 4.4450; Average accuracy: 0.0000\n",
            "Iteration: 341; Percent complete: 3.4%; Average loss: 4.6949; Average accuracy: 0.0000\n",
            "Iteration: 342; Percent complete: 3.4%; Average loss: 4.0701; Average accuracy: 0.0000\n",
            "Iteration: 343; Percent complete: 3.4%; Average loss: 4.0344; Average accuracy: 0.0000\n",
            "Iteration: 344; Percent complete: 3.4%; Average loss: 4.0344; Average accuracy: 0.0000\n",
            "Iteration: 345; Percent complete: 3.5%; Average loss: 4.9270; Average accuracy: 0.0000\n",
            "Iteration: 346; Percent complete: 3.5%; Average loss: 3.3382; Average accuracy: 0.0000\n",
            "Iteration: 347; Percent complete: 3.5%; Average loss: 4.8020; Average accuracy: 0.0000\n",
            "Iteration: 348; Percent complete: 3.5%; Average loss: 3.4453; Average accuracy: 0.0000\n",
            "Iteration: 349; Percent complete: 3.5%; Average loss: 5.0520; Average accuracy: 0.0000\n",
            "Iteration: 350; Percent complete: 3.5%; Average loss: 2.3028; Average accuracy: 0.0000\n",
            "Iteration: 351; Percent complete: 3.5%; Average loss: 3.8024; Average accuracy: 0.0000\n",
            "Iteration: 352; Percent complete: 3.5%; Average loss: 3.1954; Average accuracy: 0.0000\n",
            "Iteration: 353; Percent complete: 3.5%; Average loss: 3.1597; Average accuracy: 0.0000\n",
            "Iteration: 354; Percent complete: 3.5%; Average loss: 3.3561; Average accuracy: 0.0000\n",
            "Iteration: 355; Percent complete: 3.5%; Average loss: 4.3736; Average accuracy: 0.0000\n",
            "Iteration: 356; Percent complete: 3.6%; Average loss: 5.3554; Average accuracy: 0.0000\n",
            "Iteration: 357; Percent complete: 3.6%; Average loss: 6.4801; Average accuracy: 0.0000\n",
            "Iteration: 358; Percent complete: 3.6%; Average loss: 3.5346; Average accuracy: 0.0000\n",
            "Iteration: 359; Percent complete: 3.6%; Average loss: 4.8020; Average accuracy: 0.0000\n",
            "Iteration: 360; Percent complete: 3.6%; Average loss: 6.0873; Average accuracy: 0.0001\n",
            "Iteration: 361; Percent complete: 3.6%; Average loss: 4.0344; Average accuracy: 0.0000\n",
            "Iteration: 362; Percent complete: 3.6%; Average loss: 4.7842; Average accuracy: 0.0000\n",
            "Iteration: 363; Percent complete: 3.6%; Average loss: 3.1597; Average accuracy: 0.0000\n",
            "Iteration: 364; Percent complete: 3.6%; Average loss: 4.4093; Average accuracy: 0.0000\n",
            "Iteration: 365; Percent complete: 3.6%; Average loss: 3.7309; Average accuracy: 0.0000\n",
            "Iteration: 366; Percent complete: 3.7%; Average loss: 4.4629; Average accuracy: 0.0000\n",
            "Iteration: 367; Percent complete: 3.7%; Average loss: 3.5167; Average accuracy: 0.0000\n",
            "Iteration: 368; Percent complete: 3.7%; Average loss: 5.0163; Average accuracy: 0.0000\n",
            "Iteration: 369; Percent complete: 3.7%; Average loss: 4.0880; Average accuracy: 0.0000\n",
            "Iteration: 370; Percent complete: 3.7%; Average loss: 3.1062; Average accuracy: 0.0000\n",
            "Iteration: 371; Percent complete: 3.7%; Average loss: 4.2308; Average accuracy: 0.0000\n",
            "Iteration: 372; Percent complete: 3.7%; Average loss: 4.3379; Average accuracy: 0.0000\n",
            "Iteration: 373; Percent complete: 3.7%; Average loss: 3.9095; Average accuracy: 0.0000\n",
            "Iteration: 374; Percent complete: 3.7%; Average loss: 3.7131; Average accuracy: 0.0000\n",
            "Iteration: 375; Percent complete: 3.8%; Average loss: 2.7670; Average accuracy: 0.0000\n",
            "Iteration: 376; Percent complete: 3.8%; Average loss: 3.5346; Average accuracy: 0.0000\n",
            "Iteration: 377; Percent complete: 3.8%; Average loss: 5.0698; Average accuracy: 0.0000\n",
            "Iteration: 378; Percent complete: 3.8%; Average loss: 3.8916; Average accuracy: 0.0000\n",
            "Iteration: 379; Percent complete: 3.8%; Average loss: 3.3025; Average accuracy: 0.0000\n",
            "Iteration: 380; Percent complete: 3.8%; Average loss: 4.0344; Average accuracy: 0.0000\n",
            "Iteration: 381; Percent complete: 3.8%; Average loss: 5.4982; Average accuracy: 0.0000\n",
            "Iteration: 382; Percent complete: 3.8%; Average loss: 4.0701; Average accuracy: 0.0000\n",
            "Iteration: 383; Percent complete: 3.8%; Average loss: 3.2490; Average accuracy: 0.0000\n",
            "Iteration: 384; Percent complete: 3.8%; Average loss: 4.8377; Average accuracy: 0.0000\n",
            "Iteration: 385; Percent complete: 3.9%; Average loss: 4.9270; Average accuracy: 0.0000\n",
            "Iteration: 386; Percent complete: 3.9%; Average loss: 3.3204; Average accuracy: 0.0000\n",
            "Iteration: 387; Percent complete: 3.9%; Average loss: 4.9627; Average accuracy: 0.0000\n",
            "Iteration: 388; Percent complete: 3.9%; Average loss: 4.1058; Average accuracy: 0.0000\n",
            "Iteration: 389; Percent complete: 3.9%; Average loss: 4.6949; Average accuracy: 0.0000\n",
            "Iteration: 390; Percent complete: 3.9%; Average loss: 3.8381; Average accuracy: 0.0000\n",
            "Iteration: 391; Percent complete: 3.9%; Average loss: 3.6953; Average accuracy: 0.0000\n",
            "Iteration: 392; Percent complete: 3.9%; Average loss: 6.9442; Average accuracy: 0.0000\n",
            "Iteration: 393; Percent complete: 3.9%; Average loss: 3.0347; Average accuracy: 0.0000\n",
            "Iteration: 394; Percent complete: 3.9%; Average loss: 4.2843; Average accuracy: 0.0000\n",
            "Iteration: 395; Percent complete: 4.0%; Average loss: 3.7845; Average accuracy: 0.0000\n",
            "Iteration: 396; Percent complete: 4.0%; Average loss: 4.7663; Average accuracy: 0.0000\n",
            "Iteration: 397; Percent complete: 4.0%; Average loss: 4.9449; Average accuracy: 0.0000\n",
            "Iteration: 398; Percent complete: 4.0%; Average loss: 3.1061; Average accuracy: 0.0000\n",
            "Iteration: 399; Percent complete: 4.0%; Average loss: 4.8913; Average accuracy: 0.0000\n",
            "Iteration: 400; Percent complete: 4.0%; Average loss: 3.7488; Average accuracy: 0.0000\n",
            "Iteration: 401; Percent complete: 4.0%; Average loss: 3.1954; Average accuracy: 0.0000\n",
            "Iteration: 402; Percent complete: 4.0%; Average loss: 5.2483; Average accuracy: 0.0000\n",
            "Iteration: 403; Percent complete: 4.0%; Average loss: 5.2126; Average accuracy: 0.0000\n",
            "Iteration: 404; Percent complete: 4.0%; Average loss: 3.3025; Average accuracy: 0.0000\n",
            "Iteration: 405; Percent complete: 4.0%; Average loss: 4.3022; Average accuracy: 0.0000\n",
            "Iteration: 406; Percent complete: 4.1%; Average loss: 2.3385; Average accuracy: 0.0000\n",
            "Iteration: 407; Percent complete: 4.1%; Average loss: 3.1954; Average accuracy: 0.0000\n",
            "Iteration: 408; Percent complete: 4.1%; Average loss: 3.7310; Average accuracy: 0.0000\n",
            "Iteration: 409; Percent complete: 4.1%; Average loss: 4.1237; Average accuracy: 0.0001\n",
            "Iteration: 410; Percent complete: 4.1%; Average loss: 5.2483; Average accuracy: 0.0000\n",
            "Iteration: 411; Percent complete: 4.1%; Average loss: 4.3022; Average accuracy: 0.0000\n",
            "Iteration: 412; Percent complete: 4.1%; Average loss: 3.9095; Average accuracy: 0.0000\n",
            "Iteration: 413; Percent complete: 4.1%; Average loss: 3.7131; Average accuracy: 0.0000\n",
            "Iteration: 414; Percent complete: 4.1%; Average loss: 3.9452; Average accuracy: 0.0000\n",
            "Iteration: 415; Percent complete: 4.2%; Average loss: 4.3379; Average accuracy: 0.0000\n",
            "Iteration: 416; Percent complete: 4.2%; Average loss: 5.3733; Average accuracy: 0.0000\n",
            "Iteration: 417; Percent complete: 4.2%; Average loss: 5.4804; Average accuracy: 0.0000\n",
            "Iteration: 418; Percent complete: 4.2%; Average loss: 3.9273; Average accuracy: 0.0000\n",
            "Iteration: 419; Percent complete: 4.2%; Average loss: 5.6589; Average accuracy: 0.0004\n",
            "Iteration: 420; Percent complete: 4.2%; Average loss: 3.9987; Average accuracy: 0.0000\n",
            "Iteration: 421; Percent complete: 4.2%; Average loss: 4.0701; Average accuracy: 0.0000\n",
            "Iteration: 422; Percent complete: 4.2%; Average loss: 1.8566; Average accuracy: 0.0000\n",
            "Iteration: 423; Percent complete: 4.2%; Average loss: 3.6774; Average accuracy: 0.0000\n",
            "Iteration: 424; Percent complete: 4.2%; Average loss: 3.1062; Average accuracy: 0.0000\n",
            "Iteration: 425; Percent complete: 4.2%; Average loss: 4.1237; Average accuracy: 0.0000\n",
            "Iteration: 426; Percent complete: 4.3%; Average loss: 4.1237; Average accuracy: 0.0000\n",
            "Iteration: 427; Percent complete: 4.3%; Average loss: 3.8559; Average accuracy: 0.0000\n",
            "Iteration: 428; Percent complete: 4.3%; Average loss: 2.8384; Average accuracy: 0.0000\n",
            "Iteration: 429; Percent complete: 4.3%; Average loss: 3.3382; Average accuracy: 0.0000\n",
            "Iteration: 430; Percent complete: 4.3%; Average loss: 3.4632; Average accuracy: 0.0000\n",
            "Iteration: 431; Percent complete: 4.3%; Average loss: 3.8202; Average accuracy: 0.0000\n",
            "Iteration: 432; Percent complete: 4.3%; Average loss: 4.5521; Average accuracy: 0.0000\n",
            "Iteration: 433; Percent complete: 4.3%; Average loss: 3.4275; Average accuracy: 0.0000\n",
            "Iteration: 434; Percent complete: 4.3%; Average loss: 3.1776; Average accuracy: 0.0000\n",
            "Iteration: 435; Percent complete: 4.3%; Average loss: 3.5881; Average accuracy: 0.0000\n",
            "Iteration: 436; Percent complete: 4.4%; Average loss: 3.9987; Average accuracy: 0.0000\n",
            "Iteration: 437; Percent complete: 4.4%; Average loss: 4.6414; Average accuracy: 0.0000\n",
            "Iteration: 438; Percent complete: 4.4%; Average loss: 5.2840; Average accuracy: 0.0000\n",
            "Iteration: 439; Percent complete: 4.4%; Average loss: 3.7845; Average accuracy: 0.0000\n",
            "Iteration: 440; Percent complete: 4.4%; Average loss: 4.8020; Average accuracy: 0.0000\n",
            "Iteration: 441; Percent complete: 4.4%; Average loss: 3.5703; Average accuracy: 0.0000\n",
            "Iteration: 442; Percent complete: 4.4%; Average loss: 4.7306; Average accuracy: 0.0000\n",
            "Iteration: 443; Percent complete: 4.4%; Average loss: 5.5161; Average accuracy: 0.0000\n",
            "Iteration: 444; Percent complete: 4.4%; Average loss: 5.3019; Average accuracy: 0.0000\n",
            "Iteration: 445; Percent complete: 4.5%; Average loss: 4.8199; Average accuracy: 0.0000\n",
            "Iteration: 446; Percent complete: 4.5%; Average loss: 4.2486; Average accuracy: 0.0000\n",
            "Iteration: 447; Percent complete: 4.5%; Average loss: 3.9095; Average accuracy: 0.0000\n",
            "Iteration: 448; Percent complete: 4.5%; Average loss: 3.3739; Average accuracy: 0.0000\n",
            "Iteration: 449; Percent complete: 4.5%; Average loss: 4.6949; Average accuracy: 0.0000\n",
            "Iteration: 450; Percent complete: 4.5%; Average loss: 4.3379; Average accuracy: 0.0000\n",
            "Iteration: 451; Percent complete: 4.5%; Average loss: 3.8202; Average accuracy: 0.0000\n",
            "Iteration: 452; Percent complete: 4.5%; Average loss: 3.6595; Average accuracy: 0.0002\n",
            "Iteration: 453; Percent complete: 4.5%; Average loss: 2.7134; Average accuracy: 0.0000\n",
            "Iteration: 454; Percent complete: 4.5%; Average loss: 3.6595; Average accuracy: 0.0000\n",
            "Iteration: 455; Percent complete: 4.5%; Average loss: 3.1597; Average accuracy: 0.0000\n",
            "Iteration: 456; Percent complete: 4.6%; Average loss: 2.9990; Average accuracy: 0.0000\n",
            "Iteration: 457; Percent complete: 4.6%; Average loss: 5.3376; Average accuracy: 0.0000\n",
            "Iteration: 458; Percent complete: 4.6%; Average loss: 3.0883; Average accuracy: 0.0000\n",
            "Iteration: 459; Percent complete: 4.6%; Average loss: 4.0701; Average accuracy: 0.0000\n",
            "Iteration: 460; Percent complete: 4.6%; Average loss: 4.8020; Average accuracy: 0.0000\n",
            "Iteration: 461; Percent complete: 4.6%; Average loss: 5.0341; Average accuracy: 0.0000\n",
            "Iteration: 462; Percent complete: 4.6%; Average loss: 4.7842; Average accuracy: 0.0000\n",
            "Iteration: 463; Percent complete: 4.6%; Average loss: 4.1951; Average accuracy: 0.0000\n",
            "Iteration: 464; Percent complete: 4.6%; Average loss: 4.8377; Average accuracy: 0.0000\n",
            "Iteration: 465; Percent complete: 4.7%; Average loss: 4.6592; Average accuracy: 0.0000\n",
            "Iteration: 466; Percent complete: 4.7%; Average loss: 3.3739; Average accuracy: 0.0000\n",
            "Iteration: 467; Percent complete: 4.7%; Average loss: 4.5700; Average accuracy: 0.0000\n",
            "Iteration: 468; Percent complete: 4.7%; Average loss: 3.4810; Average accuracy: 0.0000\n",
            "Iteration: 469; Percent complete: 4.7%; Average loss: 5.2840; Average accuracy: 0.0000\n",
            "Iteration: 470; Percent complete: 4.7%; Average loss: 4.2308; Average accuracy: 0.0000\n",
            "Iteration: 471; Percent complete: 4.7%; Average loss: 5.1412; Average accuracy: 0.0000\n",
            "Iteration: 472; Percent complete: 4.7%; Average loss: 3.9273; Average accuracy: 0.0000\n",
            "Iteration: 473; Percent complete: 4.7%; Average loss: 2.9812; Average accuracy: 0.0000\n",
            "Iteration: 474; Percent complete: 4.7%; Average loss: 6.1945; Average accuracy: 0.0000\n",
            "Iteration: 475; Percent complete: 4.8%; Average loss: 2.5528; Average accuracy: 0.0000\n",
            "Iteration: 476; Percent complete: 4.8%; Average loss: 4.3022; Average accuracy: 0.0000\n",
            "Iteration: 477; Percent complete: 4.8%; Average loss: 3.0704; Average accuracy: 0.0000\n",
            "Iteration: 478; Percent complete: 4.8%; Average loss: 3.7488; Average accuracy: 0.0000\n",
            "Iteration: 479; Percent complete: 4.8%; Average loss: 4.2843; Average accuracy: 0.0000\n",
            "Iteration: 480; Percent complete: 4.8%; Average loss: 3.3025; Average accuracy: 0.0000\n",
            "Iteration: 481; Percent complete: 4.8%; Average loss: 3.8202; Average accuracy: 0.0000\n",
            "Iteration: 482; Percent complete: 4.8%; Average loss: 5.5161; Average accuracy: 0.0000\n",
            "Iteration: 483; Percent complete: 4.8%; Average loss: 3.1954; Average accuracy: 0.0000\n",
            "Iteration: 484; Percent complete: 4.8%; Average loss: 4.2486; Average accuracy: 0.0000\n",
            "Iteration: 485; Percent complete: 4.9%; Average loss: 3.5167; Average accuracy: 0.0000\n",
            "Iteration: 486; Percent complete: 4.9%; Average loss: 4.4986; Average accuracy: 0.0000\n",
            "Iteration: 487; Percent complete: 4.9%; Average loss: 2.7134; Average accuracy: 0.0000\n",
            "Iteration: 488; Percent complete: 4.9%; Average loss: 2.5706; Average accuracy: 0.0000\n",
            "Iteration: 489; Percent complete: 4.9%; Average loss: 4.3736; Average accuracy: 0.0000\n",
            "Iteration: 490; Percent complete: 4.9%; Average loss: 4.2129; Average accuracy: 0.0000\n",
            "Iteration: 491; Percent complete: 4.9%; Average loss: 3.5524; Average accuracy: 0.0000\n",
            "Iteration: 492; Percent complete: 4.9%; Average loss: 6.2659; Average accuracy: 0.0000\n",
            "Iteration: 493; Percent complete: 4.9%; Average loss: 2.6777; Average accuracy: 0.0000\n",
            "Iteration: 494; Percent complete: 4.9%; Average loss: 5.6232; Average accuracy: 0.0000\n",
            "Iteration: 495; Percent complete: 5.0%; Average loss: 3.6774; Average accuracy: 0.0000\n",
            "Iteration: 496; Percent complete: 5.0%; Average loss: 3.9452; Average accuracy: 0.0000\n",
            "Iteration: 497; Percent complete: 5.0%; Average loss: 3.6953; Average accuracy: 0.0000\n",
            "Iteration: 498; Percent complete: 5.0%; Average loss: 3.4453; Average accuracy: 0.0000\n",
            "Iteration: 499; Percent complete: 5.0%; Average loss: 3.6417; Average accuracy: 0.0000\n",
            "Iteration: 500; Percent complete: 5.0%; Average loss: 3.2847; Average accuracy: 0.0000\n",
            "Iteration: 501; Percent complete: 5.0%; Average loss: 6.0695; Average accuracy: 0.0000\n",
            "Iteration: 502; Percent complete: 5.0%; Average loss: 4.3915; Average accuracy: 0.0000\n",
            "Iteration: 503; Percent complete: 5.0%; Average loss: 4.2308; Average accuracy: 0.0000\n",
            "Iteration: 504; Percent complete: 5.0%; Average loss: 5.3554; Average accuracy: 0.0000\n",
            "Iteration: 505; Percent complete: 5.1%; Average loss: 4.5878; Average accuracy: 0.0000\n",
            "Iteration: 506; Percent complete: 5.1%; Average loss: 4.4450; Average accuracy: 0.0000\n",
            "Iteration: 507; Percent complete: 5.1%; Average loss: 2.8741; Average accuracy: 0.0000\n",
            "Iteration: 508; Percent complete: 5.1%; Average loss: 4.7842; Average accuracy: 0.0000\n",
            "Iteration: 509; Percent complete: 5.1%; Average loss: 4.9449; Average accuracy: 0.0000\n",
            "Iteration: 510; Percent complete: 5.1%; Average loss: 4.0166; Average accuracy: 0.0000\n",
            "Iteration: 511; Percent complete: 5.1%; Average loss: 4.4629; Average accuracy: 0.0000\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
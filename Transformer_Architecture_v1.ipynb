{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MessiNN/chatbot-transformer-Early_Stage-/blob/master/Transformer_Architecture_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOO0zM6eym59",
        "outputId": "1cc7519e-b047-4714-a025-5c75219bfc62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import itertools\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device_cpu = torch.device(\"cpu\")\n",
        "\n",
        "save_dir = os.path.join(\"/content/drive/MyDrive\", \"data\", \"save\")\n",
        "\n",
        "PAD_token = 0\n",
        "SOS_token = 1\n",
        "EOS_token = 2\n",
        "\n",
        "MAX_LENGTH = 20\n",
        "MIN_COUNT = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "4WP0wMu-yiqA"
      },
      "outputs": [],
      "source": [
        "class Library:\n",
        "    def __init__(self):\n",
        "        self.name = \"Dataset\"\n",
        "        self.trimmed = False\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def trim(self, min_count):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "\n",
        "        keep_words = []\n",
        "\n",
        "        for k, v in self.word2count.items():\n",
        "            if v >= min_count:\n",
        "                keep_words.append(k)\n",
        "\n",
        "\n",
        "        print(len(keep_words), len(self.word2index), len(keep_words), len(self.word2index))\n",
        "        print('keep_words {} / {} = {:.4f}'.format(\n",
        "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
        "        ))\n",
        "\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3\n",
        "\n",
        "        for word in keep_words:\n",
        "            self.addWord(word)\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def readVocs(datafile):\n",
        "    pairs = []\n",
        "    df = pd.read_parquet(datafile)\n",
        "    questions = df['question'].tolist()\n",
        "    responses = df['response'].tolist()\n",
        "    for question, response in zip(questions, responses):\n",
        "      question = normalizeString(question)\n",
        "      response = normalizeString(response)\n",
        "      pair = [question, response]\n",
        "      pairs.append(pair)\n",
        "    return pairs\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH - 1 and len(p[1].split(' ')) < MAX_LENGTH - 1\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "def loadPrepareData(datafile, save_dir):\n",
        "    libra = Library()\n",
        "    pairs = readVocs(datafile)\n",
        "    pairs = filterPairs(pairs)\n",
        "    for pair in pairs:\n",
        "        libra.addSentence(pair[0])\n",
        "        libra.addSentence(pair[1])\n",
        "    return libra, pairs\n",
        "\n",
        "\n",
        "def trimRareWords(libra, pairs, MIN_COUNT):\n",
        "    libra.trim(MIN_COUNT)\n",
        "    keep_pairs = []\n",
        "    for pair in pairs:\n",
        "        input_sentence = pair[0]\n",
        "        output_sentence = pair[1]\n",
        "        keep_input = True\n",
        "        keep_output = True\n",
        "        for word in input_sentence.split(' '):\n",
        "            if word not in libra.word2index:\n",
        "                keep_input = False\n",
        "                break\n",
        "        for word in output_sentence.split(' '):\n",
        "            if word not in libra.word2index:\n",
        "                keep_output = False\n",
        "                break\n",
        "\n",
        "        if keep_input and keep_output:\n",
        "            keep_pairs.append(pair)\n",
        "\n",
        "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
        "    return keep_pairs\n",
        "\n",
        "def indexesFromSentence(libra, sentence):\n",
        "    return [SOS_token] + [libra.word2index[word] for word in sentence.split(' ') if word in libra.word2index] + [EOS_token]\n",
        "\n",
        "def create_padding_mask(inputs):\n",
        "    padding_mask = (inputs != 0)\n",
        "    return padding_mask\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = (1 - torch.triu(torch.ones(size, size), diagonal=1)).bool()\n",
        "    return mask\n",
        "\n",
        "def zeroPadding(l, fillvalue=PAD_token):\n",
        "    padded_list = []\n",
        "    for sequence in l:\n",
        "        padded_sequence = list(sequence) + [fillvalue] * ((MAX_LENGTH) - len(sequence))\n",
        "        padded_list.append(padded_sequence)\n",
        "    return padded_list\n",
        "\n",
        "def binaryMatrix(l, value=PAD_token):\n",
        "    m = []\n",
        "    for i, seq in enumerate(l):\n",
        "        m.append([])\n",
        "        for token in seq:\n",
        "            if token == PAD_token:\n",
        "                m[i].append(0)\n",
        "            else:\n",
        "                m[i].append(1)\n",
        "    return m\n",
        "\n",
        "def inputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, lengths\n",
        "\n",
        "# Returns padded target sequence tensor, padding mask, and max target length\n",
        "def outputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    mask = binaryMatrix(padList)\n",
        "    mask = torch.BoolTensor(mask)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, mask\n",
        "\n",
        "def batch2TrainData(voc, pair_batch):\n",
        "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
        "    input_batch, output_batch = [], []\n",
        "    for pair in pair_batch:\n",
        "        input_batch.append(pair[0])\n",
        "        output_batch.append(pair[1])\n",
        "    inp, lengths = inputVar(input_batch, voc)\n",
        "    output, mask = outputVar(output_batch, voc)\n",
        "    return inp, output, lengths, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "FsqQgKMpY-GS"
      },
      "outputs": [],
      "source": [
        "#Own code implementation (By me)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, position, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.register_buffer('pos_encoding', self.positional_encoding(position, d_model))\n",
        "\n",
        "    def get_angles(self, position, i, d_model):\n",
        "        angles = 1 / torch.pow(10000, (2 * (i // 2)) / torch.FloatTensor([d_model]))\n",
        "        return position * angles\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(\n",
        "            position=torch.arange(position).unsqueeze(1),\n",
        "            i=torch.arange(d_model).unsqueeze(0),\n",
        "            d_model=d_model)\n",
        "\n",
        "        # apply sin to even indices in the array; 2i\n",
        "        sines = torch.sin(angle_rads[:, 0::2])\n",
        "        # apply cos to odd indices in the array; 2i+1\n",
        "        cosines = torch.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = torch.cat([sines, cosines], dim=-1)\n",
        "        pos_encoding = pos_encoding.unsqueeze(0)\n",
        "        return pos_encoding\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return inputs + self.pos_encoding[:, :inputs.size(1), :]\n",
        "\n",
        "\n",
        "\n",
        "class Normalize(nn.Module):\n",
        "    def __init__(self, scale: float, shift: float, epsilon: float = 1e-8):\n",
        "        super(Normalize, self).__init__()\n",
        "        self.scale = scale\n",
        "        self.shift = shift\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = torch.mean(x)\n",
        "        deviation = torch.std(x) + self.epsilon\n",
        "        x = (x - mean) / deviation\n",
        "        x = x * self.scale\n",
        "        x = x + self.shift\n",
        "        return x\n",
        "\n",
        "\n",
        "#---- Self Made Multi Head Attention Implementation ----\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, embedding_size, head_num):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.embedding_size = embedding_size\n",
        "        self.num_heads = head_num\n",
        "        self.head_size = embedding_size // self.num_heads\n",
        "\n",
        "\n",
        "        self.attn = nn.Linear(embedding_size, embedding_size)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(encoder_output)\n",
        "        energy = torch.sum(hidden * energy, dim=2)\n",
        "        return energy\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "\n",
        "        attn_energies = attn_energies.t()\n",
        "\n",
        "        attn_energies = F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
        "\n",
        "        return attn_energies\n",
        "\n",
        "\n",
        "#----Self Made Layer Encoder Implementation----\n",
        "class Encoder_Layer(nn.Module):\n",
        "    def __init__(self, embedding_size, head_num, batch_size, dropout):\n",
        "        super(Encoder_Layer, self).__init__()\n",
        "\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "        self.fc1 = nn.Linear(embedding_size*2, embedding_size)\n",
        "\n",
        "        self.norm = Normalize(0.4, 0.4)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, rnn_normalized):\n",
        "\n",
        "        #-----First Sub Layer Attention Mechanism-----#\n",
        "\n",
        "        rnn_normalized = self.dropout(rnn_normalized)\n",
        "        # each word will independently have a chance to re-initilise it's representation ( [0.1, 0.3, -0.1, 0.2] ) with all 0s\n",
        "\n",
        "        attentionNormalized = self.norm(rnn_normalized)\n",
        "        # It should be normalizing, scaling and shifting\n",
        "\n",
        "        #-----First Sub Layer Attention Mechanism-----#\n",
        "\n",
        "        return attentionNormalized\n",
        "\n",
        "\n",
        "#----Self Made Encoder Implementation----\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embedding, embedding_size, head_num, batch_size, dropout, n_layers, vocab_size):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.num_layers = n_layers\n",
        "        self.positional_encoding = PositionalEncoding(vocab_size, embedding_size)\n",
        "        self.encoder_layer = Encoder_Layer(embedding_size, head_num, batch_size, dropout)\n",
        "\n",
        "        self.embedding = embedding\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "        self.gru = nn.GRU(embedding_size, embedding_size, n_layers,\n",
        "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True, batch_first=True)\n",
        "        self.fc1 = nn.Linear(embedding_size*2, embedding_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = Normalize(0.4, 0.4)\n",
        "\n",
        "    def forward(self, source_tensor, lengths, hidden = None):\n",
        "        # shape: batch / length\n",
        "\n",
        "        source_embedding = self.embedding(source_tensor).to(device)\n",
        "        # shape: batch / length / embedding_size\n",
        "        source_embedding *= torch.sqrt(torch.tensor(self.embedding_size, dtype=torch.float32))\n",
        "        source_PosEmbedding = self.positional_encoding(source_embedding).to(device)\n",
        "\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(source_PosEmbedding, lengths, batch_first=True) # Hides paddded sequences for effiecinet computations\n",
        "        rnn_outputs, hidden = self.gru(packed, hidden)\n",
        "        rnn_outputs, _ = nn.utils.rnn.pad_packed_sequence(rnn_outputs) # Show padded sequences again\n",
        "\n",
        "        # we add pos to x's high dimensional space as context for each word position (cannot be seen as 1 more dimension)\n",
        "        # shape: batch, length / embedding_size / --high-->  pos\n",
        "        # \"What\": [0.1, 0.3, -0.1, 0.2] + Position 1: [0.1, 0.2, -0.1, -0.2] = [0.2, 0.5, -0.2, 0.0]\n",
        "\n",
        "        rnn_outputs = self.dropout(rnn_outputs).to(device)\n",
        "        rnn_normalized = self.norm(rnn_outputs).to(device)\n",
        "        # each word will independently have a chance to re-initilise it's representation ( [0.1, 0.3, -0.1, 0.2] ) with all 0s\n",
        "\n",
        "        rnn_normalized = self.encoder_layer(rnn_normalized)\n",
        "        # for each encoder_layer generate an output quite simple.\n",
        "\n",
        "        dense = self.fc1(rnn_normalized)\n",
        "\n",
        "        return dense, hidden\n",
        "\n",
        "#----Self Made Layer Decoder Implementation----\n",
        "class Decoder_Layer(nn.Module):\n",
        "    def __init__(self, embedding_size, head_num, batch_size, dropout):\n",
        "        super(Decoder_Layer, self).__init__()\n",
        "\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "        self.fc1 = nn.Linear(embedding_size, embedding_size, bias=False)\n",
        "        self.fc2 = nn.Linear(embedding_size, embedding_size, bias=False)\n",
        "\n",
        "        self.attention = Attention(embedding_size, head_num)\n",
        "\n",
        "\n",
        "        self.norm = Normalize(0.4, 0.4)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, encoder_output, hidden):\n",
        "\n",
        "        #-----First Sub Layer Attention Mechanism-----#\n",
        "\n",
        "        attn_weights = self.attention(hidden, encoder_output)\n",
        "\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        attn_weights = self.norm(attn_weights)\n",
        "        # It should be normalizing, scaling and shifting\n",
        "\n",
        "        #-----First Sub Layer Attention Mechanism-----#\n",
        "\n",
        "        return attn_weights\n",
        "\n",
        "\n",
        "#----Self Made Decoder Implementation----\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embedding, embedding_size, head_num, batch_size, dropout, n_layers, vocab_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = embedding\n",
        "        self.embedding_size = embedding_size\n",
        "        self.num_layers = n_layers\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.positional_encoding = PositionalEncoding(vocab_size, embedding_size)\n",
        "\n",
        "        self.fc = nn.Linear(embedding_size, vocab_size, bias=False)\n",
        "        self.fc2 = nn.Linear(vocab_size, embedding_size, bias=False)\n",
        "\n",
        "        self.gru = nn.GRU(embedding_size, embedding_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
        "\n",
        "        self.decoder_layer = Decoder_Layer(embedding_size, head_num, batch_size, dropout)\n",
        "        self.concat = nn.Linear(embedding_size * 2, embedding_size)\n",
        "        self.out = nn.Linear(embedding_size, vocab_size)\n",
        "\n",
        "        self.norm = Normalize(0.4, 0.4)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, decoder_input, encoder_output, hidden_inf):\n",
        "        #INPUTS\n",
        "        # decoder_input shape: batch / length\n",
        "        # enc_output shape: batch / length / embedding_size\n",
        "\n",
        "        #MASKS\n",
        "        # ahead shape: length / length\n",
        "        # mask shape: batch / length / embedding_size\n",
        "\n",
        "        inputEmbedding = self.embedding(decoder_input).to(device)\n",
        "        # shape: batch / length / embedding_size\n",
        "        inputEmbedding *= torch.sqrt(torch.tensor(self.embedding_size, dtype=torch.float32))\n",
        "        input_PosEmbedding = self.positional_encoding(inputEmbedding).to(device)\n",
        "\n",
        "        rnn_output, hidden = self.gru(input_PosEmbedding, hidden_inf)\n",
        "\n",
        "        rnn_output = self.dropout(rnn_output)\n",
        "\n",
        "        rnn_output = self.norm(rnn_output)\n",
        "\n",
        "        attn_weights = self.decoder_layer(encoder_output, rnn_output)\n",
        "\n",
        "        context = attn_weights.bmm(encoder_output.transpose(0, 1))\n",
        "\n",
        "        context = context.squeeze(1)\n",
        "\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "\n",
        "        output = self.out(concat_output)\n",
        "\n",
        "        output = F.softmax(output, dim=1)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "def maskNLLLoss(inp, target, mask):\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "    loss = loss.to(device)\n",
        "    return loss, nTotal.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "FxPpFVM2zEVM"
      },
      "outputs": [],
      "source": [
        "def train(input_variable, target_variable, vocab_size, decoder, encoder, clip,\n",
        "          encoder_optimizer, decoder_optimizer, embedding_size,head_num, batch_size, lengths, mask, max_length=MAX_LENGTH):\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "\n",
        "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]]).to(device)\n",
        "\n",
        "    encoder_output, hidden = encoder(input_variable, lengths)\n",
        "    decoder_hidden = hidden[:decoder.num_layers]\n",
        "\n",
        "    choice = random.random()\n",
        "\n",
        "\n",
        "    if choice > 0.5:\n",
        "      use_teacher_forcing = True\n",
        "    else:\n",
        "      use_teacher_forcing = False\n",
        "\n",
        "    target_variable = target_variable.t()\n",
        "    mask = mask.t()\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, encoder_output, decoder_hidden\n",
        "            )\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "\n",
        "            mask_loss, n_total = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * n_total)\n",
        "            n_totals += n_total\n",
        "\n",
        "    elif use_teacher_forcing == False:\n",
        "        for t in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, encoder_output, decoder_hidden\n",
        "            )\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            decoder_input = decoder_input.to(device)\n",
        "\n",
        "            mask_loss, n_total = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * n_total)\n",
        "            n_totals += n_total\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return sum(print_losses) / n_totals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "cvybKce03AI1"
      },
      "outputs": [],
      "source": [
        "def trainIters(model_name, libra, pairs, save_dir, n_iteration, batch_size, checkpoint, clip,\n",
        "               print_every, save_every, loadFilename, vocab_size, decoder, encoder, head_num, dropout,\n",
        "               decoder_optimizer, encoder_optimizer, embedding, embedding_size):\n",
        "\n",
        "    print(\"Creating the training batches...\")\n",
        "    training_pairs = [batch2TrainData(libra,[random.choice(pairs) for _ in range(batch_size)])\n",
        "                      for _ in range(n_iteration)]\n",
        "\n",
        "    start_iteration = 1\n",
        "    tries = 0\n",
        "\n",
        "    if loadFilename:\n",
        "        tries = checkpoint['time']\n",
        "\n",
        "\n",
        "    print(\"Initializing Training...\")\n",
        "    print()\n",
        "    for iteration in range(start_iteration, n_iteration + 1):\n",
        "        training_pair = training_pairs[iteration - 1]\n",
        "\n",
        "        input_variable, target_variable, lengths, mask = training_pair\n",
        "        # batch / length\n",
        "\n",
        "        input_variable = input_variable.to(device)\n",
        "        target_variable = target_variable.to(device)\n",
        "\n",
        "        sum_loss = train(input_variable, target_variable, vocab_size, decoder, encoder, clip,\n",
        "          encoder_optimizer, decoder_optimizer, embedding_size, head_num, batch_size, lengths, mask)\n",
        "\n",
        "        if iteration % print_every == 0:\n",
        "            print(\"Epoch [{}/{}]\\tLoss: {:.3f}\".format(iteration, n_iteration,  sum_loss))\n",
        "\n",
        "\n",
        "        if (iteration % save_every == 0):\n",
        "                    tries += save_every\n",
        "                    directory = os.path.join(save_dir, model_name, '{}-{}_{}'.format(embedding_size, head_num, vocab_size))\n",
        "                    if not os.path.exists(directory):\n",
        "                        os.makedirs(directory)\n",
        "                    torch.save({\n",
        "                        'iteration': iteration,\n",
        "                        'time': tries,\n",
        "                        'en': encoder.state_dict(),\n",
        "                        'de': decoder.state_dict(),\n",
        "                        'en_opt': encoder_optimizer.state_dict(),\n",
        "                        'de_opt': decoder_optimizer.state_dict(),\n",
        "                        'loss': sum_loss,\n",
        "                        'voc_dict': libra.__dict__,\n",
        "                        'embedding': embedding.state_dict()\n",
        "                    }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "l_U2XyuVzJJN"
      },
      "outputs": [],
      "source": [
        "def evaluate(input_sentence, decoder, encoder, libra, embedding_size):\n",
        "    # Tokenize the sentence\n",
        "    indexedSequence = [indexesFromSentence(libra, input_sentence)]\n",
        "    paddedSequence = zeroPadding(indexedSequence)\n",
        "    padded_tensor = torch.LongTensor(paddedSequence)\n",
        "\n",
        "    padding_mask = create_padding_mask(padded_tensor)\n",
        "\n",
        "    # Convert to tensor and add batch dimension\n",
        "    sentence_tensor = torch.tensor(padded_tensor, dtype=torch.long)\n",
        "\n",
        "    # Initialize output tensor with start token\n",
        "    decoder_input = torch.LongTensor([SOS_token for _ in range(1)])\n",
        "    decoder_input = decoder_input.t()\n",
        "\n",
        "    encoder_output = encoder(sentence_tensor, padding_mask)\n",
        "\n",
        "    for i in range(MAX_LENGTH):\n",
        "        with torch.no_grad():\n",
        "            decoder_output = decoder(decoder_input, encoder_output, padding_mask)\n",
        "\n",
        "        predicted_id = torch.argmax(decoder_output, axis=-1)\n",
        "\n",
        "        decoder_input = predicted_id\n",
        "\n",
        "    return decoder_input.squeeze(0)\n",
        "\n",
        "\n",
        "def predict(input_sentence, decoder, encoder, libra, embedding_size):\n",
        "    prediction = evaluate(input_sentence, decoder, encoder, libra, embedding_size)\n",
        "    predicted_sentence = [libra.index2word[index.item()] for index in prediction if index.item() < libra.num_words]\n",
        "    return predicted_sentence\n",
        "\n",
        "\n",
        "def evaluateInput(decoder, encoder, libra, embedding_size):\n",
        "    input_sentence = ''\n",
        "    while(1):\n",
        "        try:\n",
        "            input_sentence = input('User > ')\n",
        "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
        "            input_sentence = normalizeString(input_sentence)\n",
        "            output_words = predict(input_sentence, decoder, encoder, libra, embedding_size)\n",
        "            print('Cleopatra:', ' '.join(output_words))\n",
        "        except KeyError:\n",
        "            print(\"Error: Encountered unknown word.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWrdvnJ4yxW8",
        "outputId": "d3f38663-4388-40e7-d769-3d8cad1bd86c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2435 10293 2435 10293\n",
            "keep_words 2435 / 10293 = 0.2366\n",
            "Trimmed from 2661 pairs to 340, 0.1278 of total\n"
          ]
        }
      ],
      "source": [
        "parquet_path = \"/content/drive/MyDrive/movie-corpus/movie-corpus/0000.parquet\"\n",
        "libra, pairs = loadPrepareData(parquet_path, save_dir)\n",
        "pairs = trimRareWords(libra, pairs, MIN_COUNT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "dIU-IR9BzVUH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 981
        },
        "outputId": "ab4e9e7b-690d-448f-b6bb-774ae013b80e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set to: 'new model'\n",
            "Creating the training batches...\n",
            "Initializing Training...\n",
            "\n",
            "Epoch [1/1000]\tLoss: nan\n",
            "Epoch [2/1000]\tLoss: nan\n",
            "Epoch [3/1000]\tLoss: nan\n",
            "Epoch [4/1000]\tLoss: nan\n",
            "Epoch [5/1000]\tLoss: 7.669\n",
            "Epoch [6/1000]\tLoss: 7.642\n",
            "Epoch [7/1000]\tLoss: 7.610\n",
            "Epoch [8/1000]\tLoss: nan\n",
            "Epoch [9/1000]\tLoss: nan\n",
            "Epoch [10/1000]\tLoss: 7.382\n",
            "Epoch [11/1000]\tLoss: 7.243\n",
            "Epoch [12/1000]\tLoss: 7.349\n",
            "Epoch [13/1000]\tLoss: nan\n",
            "Epoch [14/1000]\tLoss: nan\n",
            "Epoch [15/1000]\tLoss: nan\n",
            "Epoch [16/1000]\tLoss: nan\n",
            "Epoch [17/1000]\tLoss: nan\n",
            "Epoch [18/1000]\tLoss: 6.550\n",
            "Epoch [19/1000]\tLoss: 6.440\n",
            "Epoch [20/1000]\tLoss: 6.409\n",
            "Epoch [21/1000]\tLoss: 6.480\n",
            "Epoch [22/1000]\tLoss: nan\n",
            "Epoch [23/1000]\tLoss: nan\n",
            "Epoch [24/1000]\tLoss: nan\n",
            "Epoch [25/1000]\tLoss: nan\n",
            "Epoch [26/1000]\tLoss: 6.317\n",
            "Epoch [27/1000]\tLoss: nan\n",
            "Epoch [28/1000]\tLoss: nan\n",
            "Epoch [29/1000]\tLoss: 5.662\n",
            "Epoch [30/1000]\tLoss: nan\n",
            "Epoch [31/1000]\tLoss: 6.170\n",
            "Epoch [32/1000]\tLoss: 5.954\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-af235216979f>\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     trainIters(model_name, libra, pairs, save_dir, n_iteration, batch_size, checkpoint, clip,\n\u001b[0m\u001b[1;32m     73\u001b[0m                \u001b[0mprint_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloadFilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                decoder_optimizer, encoder_optimizer, embedding, embedding_size)\n",
            "\u001b[0;32m<ipython-input-88-f4482a2d573b>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(model_name, libra, pairs, save_dir, n_iteration, batch_size, checkpoint, clip, print_every, save_every, loadFilename, vocab_size, decoder, encoder, head_num, dropout, decoder_optimizer, encoder_optimizer, embedding, embedding_size)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtarget_variable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_variable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         sum_loss = train(input_variable, target_variable, vocab_size, decoder, encoder, clip,\n\u001b[0m\u001b[1;32m     28\u001b[0m           encoder_optimizer, decoder_optimizer, embedding_size, head_num, batch_size, lengths, mask)\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-92-70ce7cb54d73>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_variable, target_variable, vocab_size, decoder, encoder, clip, encoder_optimizer, decoder_optimizer, embedding_size, head_num, batch_size, lengths, mask, max_length)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mn_totals\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mn_total\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model_name = 'Cleopatra_model'\n",
        "checkpoint=None\n",
        "start_model = \"no\"\n",
        "loadFilename = None if start_model == \"no\" else \"/content/drive/MyDrive/data/save/Cleopatra_model/256-4_5442/20000_checkpoint.tar\"\n",
        "\n",
        "if loadFilename:\n",
        "    print(\"Set to: 'trained model'\")\n",
        "    checkpoint = torch.load(loadFilename, map_location=device)\n",
        "    encoder_sd = checkpoint['en']\n",
        "    decoder_sd = checkpoint['de']\n",
        "    encoder_optimizer_sd = checkpoint['en_opt']\n",
        "    decoder_optimizer_sd = checkpoint['de_opt']\n",
        "    embedding_sd = checkpoint['embedding']\n",
        "    libra.__dict__ = checkpoint['voc_dict']\n",
        "    print(\"Loss: \",checkpoint[\"loss\"])\n",
        "    print(\"Time: \",checkpoint[\"time\"])\n",
        "else:\n",
        "    print(\"Set to: 'new model'\")\n",
        "\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 4\n",
        "embedding_size = 512\n",
        "head_num = 8\n",
        "\n",
        "if embedding_size % head_num != 0:\n",
        "    raise ValueError(\"embedding_size / head_num must result in an integer\")\n",
        "\n",
        "dropout = 0.05\n",
        "batch_size = 10\n",
        "learning_rate = 0.0001\n",
        "vocab_size = libra.num_words\n",
        "\n",
        "task = \"train\"\n",
        "\n",
        "\n",
        "embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "decoder = Decoder(embedding, embedding_size, head_num, batch_size, dropout, decoder_n_layers, vocab_size)\n",
        "encoder = Encoder(embedding, embedding_size, head_num, batch_size, dropout, encoder_n_layers, vocab_size)\n",
        "\n",
        "\n",
        "if loadFilename:\n",
        "    embedding.load_state_dict(embedding_sd)\n",
        "\n",
        "if loadFilename:\n",
        "    encoder.load_state_dict(encoder_sd)\n",
        "    decoder.load_state_dict(decoder_sd)\n",
        "\n",
        "embedding = embedding.to(device)\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "\n",
        "if task == \"train\":\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "else:\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "decoder_optimizer = optim.AdamW(decoder.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-8)\n",
        "encoder_optimizer = optim.AdamW(encoder.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-8)\n",
        "\n",
        "if loadFilename:\n",
        "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
        "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
        "\n",
        "clip = 5.0\n",
        "n_iteration = 1000\n",
        "print_every = 1\n",
        "save_every = 10000\n",
        "\n",
        "if task == \"train\":\n",
        "    trainIters(model_name, libra, pairs, save_dir, n_iteration, batch_size, checkpoint, clip,\n",
        "               print_every, save_every, loadFilename, vocab_size, decoder, encoder, head_num, dropout,\n",
        "               decoder_optimizer, encoder_optimizer, embedding, embedding_size)\n",
        "\n",
        "if task == \"test\":\n",
        "    evaluateInput(decoder, encoder, libra, embedding_size)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MessiNN/chatbot-transformer-Early_Stage-/blob/master/Transformer_Architecture_v1_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOO0zM6eym59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bec3313a-934f-4648-e659-aeed9f500cc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import yaml\n",
        "import csv\n",
        "import unicodedata\n",
        "import itertools\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device_cpu = torch.device(\"cpu\")\n",
        "\n",
        "save_dir = os.path.join(\"/content/drive/MyDrive\", \"data\", \"save\")\n",
        "\n",
        "PAD_token = 0\n",
        "SOS_token = 1\n",
        "EOS_token = 2\n",
        "\n",
        "MAX_LENGTH = 40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WP0wMu-yiqA"
      },
      "outputs": [],
      "source": [
        "class Library:\n",
        "    def __init__(self):\n",
        "        self.name = \"Dataset\"\n",
        "        self.trimmed = False\n",
        "        self.word2index = {\"PAD\": PAD_token, \"SOS\": SOS_token, \"EOS\": EOS_token}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "# We are changing the Standard from unicode (Global Standard) to ASCII which is more in line with our occidental vocabulary (American Standard).\n",
        "#Thus avoiding letters like \"你好，안녕하세요，こんにちは\" but allowing letters from A-Z.\n",
        "def unicodeToAscii(string):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', string)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "\n",
        "def normalizeString(sentence):\n",
        "    if isinstance(sentence, dict):\n",
        "      sentence = f\"{sentence}\"\n",
        "    AsciiSentence = unicodeToAscii(sentence.lower().strip())\n",
        "    AsciiSentence = re.sub(r\"([.!?])\", r\" \", AsciiSentence)\n",
        "    AsciiSentence = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", AsciiSentence)\n",
        "    AsciiSentence = re.sub(r\"[^\\x00-\\x7F]\", r\"\", AsciiSentence) # This is basically the same as the Unicode to Ascii\n",
        "    AsciiSentence = re.sub(r\"\\s+\", r\" \", AsciiSentence).strip()\n",
        "    return AsciiSentence\n",
        "\n",
        "def removePair(pairs):\n",
        "    try:\n",
        "        filtered_pairs = [[left, right] for left, right in pairs if left and right]\n",
        "    except:\n",
        "        filtered_pairs = [[pair[0], pair[1]] for pair in pairs if pair[0] and pair[1]]\n",
        "    return filtered_pairs\n",
        "\n",
        "def readCsv(datafile):\n",
        "    pairs = []\n",
        "    questions = []\n",
        "    responses = []\n",
        "    with open(datafile, 'r') as csv_file:\n",
        "        csv_reader = csv.reader(csv_file)\n",
        "        next(csv_reader)\n",
        "        for row in csv_reader:\n",
        "            if len(row) >= 2:\n",
        "                questions.append(row[1])\n",
        "                responses.append(row[2])\n",
        "    for question, response in zip(questions, responses):\n",
        "        question = normalizeString(question)\n",
        "        response = normalizeString(response)\n",
        "        pair = [question, response]\n",
        "        pairs.append(pair)\n",
        "    filtered_pairs = removePair(pairs)\n",
        "    return filtered_pairs\n",
        "\n",
        "\n",
        "def readPanda(datafile):\n",
        "    pairs = []\n",
        "    dataset = pd.read_parquet(datafile)\n",
        "    questions = dataset['question'].tolist()\n",
        "    responses = dataset['response'].tolist()\n",
        "    for question, response in zip(questions, responses):\n",
        "        question = normalizeString(question)\n",
        "        response = normalizeString(response)\n",
        "        pair = [question, response]\n",
        "        pairs.append(pair)\n",
        "    filtered_pairs = removePair(pairs)\n",
        "    return filtered_pairs\n",
        "\n",
        "def readTxt(datafile):\n",
        "    lines = open(datafile, encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    filtered_pairs = removePair(pairs)\n",
        "    return  filtered_pairs\n",
        "\n",
        "def readYml(datafile):\n",
        "    pairs = []\n",
        "    with open(datafile, \"r\") as file:\n",
        "        yaml_content = file.read()\n",
        "    data = yaml.safe_load(yaml_content)\n",
        "    for sentences in data.get(\"conversations\", []):\n",
        "        question = sentences[0]\n",
        "        sentences.pop(0)\n",
        "        if len(sentences) > 1:\n",
        "          for answer in sentences:\n",
        "              question = normalizeString(question)\n",
        "              answer = normalizeString(answer)\n",
        "              pairs.append([question, answer])\n",
        "        if len(sentences) == 1:\n",
        "              question = normalizeString(question)\n",
        "              sentences = normalizeString(sentences[0])\n",
        "              pairs.append([question, sentences])\n",
        "    return pairs\n",
        "\n",
        "def customData(datafile):\n",
        "    pairs = []\n",
        "    pair = []\n",
        "    data = open(datafile, encoding='utf-8').\\\n",
        "        read().strip().split(\"\\n\")\n",
        "    for line in data:\n",
        "        split_strings = line.split(\"\\\\t\")\n",
        "        source = normalizeString(split_strings[0])\n",
        "        target = normalizeString(split_strings[1])\n",
        "        pair.append(source)\n",
        "        pair.append(target)\n",
        "        pairs.append(pair)\n",
        "        pair = []\n",
        "    filtrated_pairs = removePair(pairs)\n",
        "    return filtrated_pairs\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH - 1 and len(p[1].split(' ')) < MAX_LENGTH - 1\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "def loadPrepareData(libra, option):\n",
        "    if option == 1:\n",
        "        pairs = readPanda(\"/content/drive/MyDrive/movie-corpus/movie-corpus/0000.parquet\")\n",
        "    elif option == 2:\n",
        "        pairs = readCsv(\"/content/drive/MyDrive/movie-corpus/movie-corpus/Conversation.csv\")\n",
        "    elif option == 3:\n",
        "        pairs = readTxt(\"/content/drive/MyDrive/movie-corpus/movie-corpus/formatted_movie_lines.txt\")\n",
        "    elif option == 4:\n",
        "        pairs = customData(\"/content/drive/MyDrive/movie-corpus/movie-corpus/customdata.txt\")\n",
        "    elif option == 5:\n",
        "        pairs = readYml(\"/content/drive/MyDrive/movie-corpus/movie-corpus/Topics/full/alltopics.yml\")\n",
        "    else:\n",
        "        fullPairs = []\n",
        "        pairs = []\n",
        "\n",
        "        pairs1 = readTxt(\"/content/drive/MyDrive/movie-corpus/movie-corpus/formatted_movie_lines.txt\")\n",
        "        fullPairs.append(pairs1)\n",
        "\n",
        "        pairs2 = readCsv(\"/content/drive/MyDrive/movie-corpus/movie-corpus/Conversation.csv\")\n",
        "        fullPairs.append(pairs2)\n",
        "\n",
        "        pairs3 = readPanda(\"/content/drive/MyDrive/movie-corpus/movie-corpus/0000.parquet\")\n",
        "        fullPairs.append(pairs3)\n",
        "\n",
        "        pairs4 = customData(\"/content/drive/MyDrive/movie-corpus/movie-corpus/customdata.txt\")\n",
        "        fullPairs.append(pairs4)\n",
        "\n",
        "        pairs5 = readYml(\"/content/drive/MyDrive/movie-corpus/movie-corpus/Topics/full/alltopics.yml\")\n",
        "        fullPairs.append(pairs5)\n",
        "\n",
        "        for p in fullPairs:\n",
        "            for pair in p:\n",
        "                pairs.append(pair)\n",
        "\n",
        "    for pair in pairs:\n",
        "        libra.addSentence(pair[0])\n",
        "        libra.addSentence(pair[1])\n",
        "\n",
        "    pairs = filterPairs(pairs)\n",
        "\n",
        "    return pairs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def SentenceToNum(libra, sentence):\n",
        "    return [SOS_token] + [libra.word2index[word] for word in sentence.split(' ') if word in libra.word2index] + [EOS_token]\n",
        "\n",
        "def Padding(batch):\n",
        "    padded_list = []\n",
        "    for sequence in batch:\n",
        "        padded_sequence = list(sequence) + [PAD_token] * ((MAX_LENGTH) - len(sequence))\n",
        "        padded_list.append(padded_sequence)\n",
        "    return padded_list\n",
        "\n",
        "\n",
        "def BinaryMask(batch):\n",
        "    binary = []\n",
        "    for i, seq in enumerate(batch):\n",
        "        binary.append([])\n",
        "        for token in seq:\n",
        "            if token == PAD_token:\n",
        "                binary[i].append(0)\n",
        "            else:\n",
        "                binary[i].append(1)\n",
        "    return binary\n",
        "\n",
        "def inputVar(batch, libra):\n",
        "    indexes_batch = [SentenceToNum(libra, sentence) for sentence in batch] # batch of tokenized sentences\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    padList = Padding(indexes_batch)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, lengths\n",
        "\n",
        "# Returns padded target sequence tensor, padding mask, and max target length\n",
        "def outputVar(batch, libra):\n",
        "    indexes_batch = [SentenceToNum(libra, sentence) for sentence in batch]\n",
        "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
        "    padList = Padding(indexes_batch)\n",
        "    mask = BinaryMask(padList)\n",
        "    mask = torch.BoolTensor(mask)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, mask, max_target_len\n",
        "\n",
        "def batch2TrainData(libra, pair_batch):\n",
        "    input_batch, output_batch = [], []\n",
        "    for pair in pair_batch:\n",
        "        input_batch.append(pair[0])\n",
        "        output_batch.append(pair[1])\n",
        "    inp, lengths = inputVar(input_batch, libra)\n",
        "    output, mask, max_target_len = outputVar(output_batch, libra)\n",
        "    return inp, output, lengths, mask, max_target_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsqQgKMpY-GS"
      },
      "outputs": [],
      "source": [
        "# Normalization Module\n",
        "class Normalization(nn.Module):\n",
        "    def __init__(self, scale: float, shift: float, epsilon: float = 1e-8):\n",
        "        super(Normalization, self).__init__()\n",
        "        self.scale = scale\n",
        "        self.shift = shift\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = torch.mean(x)\n",
        "        deviation = torch.std(x) + self.epsilon\n",
        "        x = (x - mean) / deviation\n",
        "        x = x * self.scale\n",
        "        x = x + self.shift\n",
        "        return x\n",
        "\n",
        "#Special_Neurons # We need to increase the number of neurons\n",
        "class DecoderNeurons(nn.Module):\n",
        "    def __init__(self, embedding_size: int, vocab_size: int):\n",
        "        super(DecoderNeurons, self).__init__()\n",
        "        self.fc1 = nn.Linear(embedding_size * 2, embedding_size)\n",
        "        self.fc2 = nn.Linear(embedding_size, vocab_size)\n",
        "        self.tnh = nn.Tanh()\n",
        "        self.sig = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        #x = self.sig(x)\n",
        "        x = self.tnh(self.fc1(x))\n",
        "        output = self.fc2(x)\n",
        "        return output\n",
        "\n",
        "class EncoderNeurons(nn.Module):\n",
        "    def __init__(self, embedding_size: int):\n",
        "        super(EncoderNeurons, self).__init__()\n",
        "        self.fc1 = nn.Linear(embedding_size * 2, embedding_size)\n",
        "        self.fc2 = nn.Linear(embedding_size, embedding_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        x = self.sig(self.fc1(x))\n",
        "        output = self.fc2(x)\n",
        "        return output\n",
        "\n",
        "# Attention Module\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, embedding_size: int):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attn = nn.Linear(embedding_size, embedding_size)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(encoder_output)\n",
        "        energy = torch.sum(hidden * energy, dim=2)\n",
        "        return energy\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "        attn_energies = attn_energies.t()\n",
        "        attn_energies = F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
        "        return attn_energies\n",
        "\n",
        "# Encoder Layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, embedding_size: int, dropout: float):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.network = EncoderNeurons(embedding_size)\n",
        "        self.norm = Normalization(0.4, 0.4)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, rnn_output):\n",
        "        input_dropped = self.dropout(rnn_output)\n",
        "        input_normalized = self.norm(input_dropped)\n",
        "        output = self.network(input_normalized)\n",
        "        return output\n",
        "\n",
        "# Encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embedding, embedding_size: int, dropout: float, n_layers: int):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.num_layers = n_layers\n",
        "        self.encoder_layer = EncoderLayer(embedding_size, dropout)\n",
        "        self.embedding = embedding\n",
        "        self.embedding_size = embedding_size\n",
        "        self.lstm = nn.LSTM(embedding_size, embedding_size, n_layers,\n",
        "                             dropout=dropout, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, source_tensor, lengths):\n",
        "        source_embedding = self.embedding(source_tensor).to(device)\n",
        "        source_embedding = self.dropout(source_embedding).to(device)\n",
        "        rnn_output, hidden = self.lstm(source_embedding)\n",
        "        output = self.encoder_layer(rnn_output)\n",
        "        return output, hidden\n",
        "\n",
        "# Decoder Layer\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embedding_size: int, prediction_size: int, dropout: float):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.norm = Normalization(0.4, 0.4)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.network = DecoderNeurons(embedding_size, prediction_size)\n",
        "\n",
        "    def forward(self, concat_input):\n",
        "        input_dropped = self.dropout(concat_input)\n",
        "        input_normalized = self.norm(input_dropped)\n",
        "        output = self.network(input_normalized)\n",
        "        return output\n",
        "\n",
        "# Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embedding, embedding_size: int, dropout: float, n_layers: int, prediction_size: int):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.num_layers = n_layers\n",
        "\n",
        "        self.attention = Attention(embedding_size)\n",
        "        self.decoder_layer = DecoderLayer(embedding_size, prediction_size, dropout)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_size, embedding_size, n_layers, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.embedding = embedding\n",
        "\n",
        "    def forward(self, decoder_input, encoder_output, hidden_inf):\n",
        "        inputEmbedding = self.embedding(decoder_input)\n",
        "        inputEmbedding = self.dropout(inputEmbedding)\n",
        "        rnn_output, hidden = self.lstm(inputEmbedding, hidden_inf)\n",
        "        attn_weights = self.attention(rnn_output, encoder_output)\n",
        "        context = attn_weights.bmm(encoder_output.transpose(0, 1))\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        network_output = self.decoder_layer(concat_input)\n",
        "        output = F.softmax(network_output, dim=1)\n",
        "        return output, hidden\n",
        "\n",
        "# Loss Function\n",
        "def maskNLLLoss(inp, target, mask):\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "    loss = loss.to(device)\n",
        "    return loss, nTotal.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxPpFVM2zEVM"
      },
      "outputs": [],
      "source": [
        "def train(input_variable, target_variable, decoder, encoder, clip, libra,\n",
        "          encoder_optimizer, decoder_optimizer, batch_size, lengths, mask, max_length, iteration):\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "\n",
        "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]]).to(device)\n",
        "\n",
        "    encoder_output, hidden = encoder(input_variable.t(), lengths)\n",
        "    (hidden_state, cell_state) = hidden\n",
        "    decoder_hidden = (hidden_state[:decoder.num_layers], cell_state[:decoder.num_layers])\n",
        "\n",
        "    choice = random.random()\n",
        "\n",
        "\n",
        "    if choice > 0.5:\n",
        "      use_teacher_forcing = True\n",
        "    else:\n",
        "      use_teacher_forcing = False\n",
        "\n",
        "    target_variable = target_variable.t()\n",
        "    mask = mask.t()\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        #all_tokens = torch.zeros([0], device=device_cpu, dtype=torch.long).to(device)\n",
        "        for t in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input.to(device), encoder_output, decoder_hidden\n",
        "            )\n",
        "\n",
        "            #decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "            #all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            mask_loss, n_total = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * n_total)\n",
        "            n_totals += n_total\n",
        "        #print(1,\"\\n\")\n",
        "\n",
        "    elif use_teacher_forcing == False:\n",
        "        #all_tokens = torch.zeros([0], device=device_cpu, dtype=torch.long).to(device)\n",
        "        for t in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, encoder_output, decoder_hidden\n",
        "            )\n",
        "\n",
        "            #decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "            #all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "\n",
        "\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]]).to(device)\n",
        "            mask_loss, n_total = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * n_total)\n",
        "            n_totals += n_total\n",
        "        #print(2,\"\\n\")\n",
        "    #decoded_words = [libra.index2word[token.item()] for token in all_tokens]\n",
        "    #target_words = [libra.index2word[token.item()] for token in target_variable]\n",
        "    #decoded_words[:] = [x for x in decoded_words if not (x == 'EOS' or x == 'SOS')]\n",
        "    #target_words[:] = [x for x in target_words if not (x == 'EOS' or x == 'SOS')]\n",
        "    #print('Cleopatra:', ' '.join(decoded_words))\n",
        "    #print('Target:', ' '.join(target_words))\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    if iteration % print_every == 0:\n",
        "      loss_avg = sum(print_losses) / n_totals / print_every\n",
        "      print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, loss_avg))\n",
        "      print_loss = 0\n",
        "\n",
        "    return loss_avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvybKce03AI1"
      },
      "outputs": [],
      "source": [
        "def trainIters(model_name, libra, save_dir, n_iteration, batch_size, checkpoint, clip,\n",
        "               print_every, save_every, loadFilename, prediction_size, decoder, encoder, dropout,\n",
        "               decoder_optimizer, encoder_optimizer, embedding, embedding_size, pairs, times):\n",
        "\n",
        "    training_pairs = [batch2TrainData(libra, [random.choice(pairs) for _ in range(batch_size)])\n",
        "                        for _ in range(n_iteration)]\n",
        "\n",
        "    start_iteration = 1\n",
        "    print_loss = 0\n",
        "    tries = 0\n",
        "\n",
        "    if loadFilename:\n",
        "        tries = checkpoint['time']\n",
        "\n",
        "\n",
        "    print(\"Initializing Training...\")\n",
        "    print()\n",
        "    for interlatitude in range(times):\n",
        "      for iteration in range(start_iteration, n_iteration + 1):\n",
        "          training_pair = training_pairs[iteration - 1]\n",
        "\n",
        "          input_variable, target_variable, lengths, mask, max_target_len = training_pair\n",
        "          # batch / length\n",
        "\n",
        "          input_variable = input_variable.to(device)\n",
        "          target_variable = target_variable.to(device)\n",
        "          mask = mask.to(device)\n",
        "\n",
        "          while(1):\n",
        "              loss = train(input_variable, target_variable, decoder, encoder, clip, libra,\n",
        "                         encoder_optimizer, decoder_optimizer, batch_size, lengths, mask, max_target_len, iteration)\n",
        "              if loss <= 0.05:\n",
        "                break\n",
        "\n",
        "          if (iteration % save_every == 0):\n",
        "                      tries += save_every\n",
        "                      directory = os.path.join(save_dir, model_name, 'Words-{}'.format(prediction_size))\n",
        "                      if not os.path.exists(directory):\n",
        "                          os.makedirs(directory)\n",
        "                      torch.save({\n",
        "                          'iteration': iteration,\n",
        "                          'time': tries,\n",
        "                          'en': encoder.state_dict(),\n",
        "                          'de': decoder.state_dict(),\n",
        "                          'en_opt': encoder_optimizer.state_dict(),\n",
        "                          'de_opt': decoder_optimizer.state_dict(),\n",
        "                          'loss': loss,\n",
        "                          'voc_dict': libra.__dict__,\n",
        "                          'embedding': embedding.state_dict()\n",
        "                      }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_U2XyuVzJJN"
      },
      "outputs": [],
      "source": [
        "class GreedySearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder, libra):\n",
        "        super(GreedySearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.libra = libra\n",
        "\n",
        "    def forward(self, input_sentence):\n",
        "\n",
        "        all_tokens = torch.zeros([0], device=device_cpu, dtype=torch.long)\n",
        "        all_scores = torch.zeros([0], device=device_cpu)\n",
        "\n",
        "        indexedSequence = [SentenceToNum(self.libra, input_sentence)]\n",
        "        lengths = torch.tensor([len(indexes) for indexes in indexedSequence])\n",
        "\n",
        "\n",
        "        paddedSequence = Padding(indexedSequence)\n",
        "        padded_tensor = torch.LongTensor(paddedSequence)\n",
        "        padding_mask = (padded_tensor != 0)\n",
        "\n",
        "\n",
        "        # Convert to tensor and add batch dimension\n",
        "        sentence_tensor = torch.LongTensor(padded_tensor).t()\n",
        "\n",
        "\n",
        "        # Initialize output tensor with start token\n",
        "        decoder_input = torch.LongTensor([[SOS_token]])\n",
        "        decoder_input = decoder_input.t()\n",
        "\n",
        "        encoder_output, hidden = self.encoder(sentence_tensor.to(device_cpu), lengths.to(device_cpu))\n",
        "        (hidden_state, cell_state) = hidden\n",
        "        decoder_hidden = (hidden_state[:self.decoder.num_layers], cell_state[:self.decoder.num_layers])\n",
        "\n",
        "        for _ in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden = self.decoder(\n",
        "                decoder_input, encoder_output, decoder_hidden\n",
        "            )\n",
        "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "\n",
        "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "\n",
        "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "\n",
        "\n",
        "        decoded_words = [libra.index2word[token.item()] for token in all_tokens]\n",
        "\n",
        "        return decoded_words\n",
        "\n",
        "class BeamSearch(nn.Module):\n",
        "    def __init__(self, beam_width, encoder, decoder, libra, temp= 0.5, penalty = 0.2):\n",
        "        super(BeamSearch, self).__init__()\n",
        "        self.beam_width = beam_width\n",
        "        self.libra = libra\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.temperature = temp\n",
        "        self.rep_penalty = penalty\n",
        "\n",
        "    def repetition(self, sequence):\n",
        "        token_counts = {}\n",
        "        penalty = 0\n",
        "        for token in sequence:\n",
        "            if token in token_counts:\n",
        "                penalty += token_counts[token]\n",
        "                token_counts[token] += 1\n",
        "            else:\n",
        "                token_counts[token] = 1\n",
        "        return penalty * self.rep_penalty\n",
        "\n",
        "    def forward(self, input_sentence):\n",
        "        indexedSequence = [SentenceToNum(self.libra, input_sentence)]\n",
        "        lengths = torch.tensor([len(indexes) for indexes in indexedSequence])\n",
        "\n",
        "        paddedSequence = Padding(indexedSequence)\n",
        "        sentence_tensor = torch.LongTensor(paddedSequence).t()\n",
        "\n",
        "        decoder_input = torch.LongTensor([[SOS_token]]).t()\n",
        "\n",
        "        encoder_output, hidden = self.encoder(sentence_tensor.to(device_cpu), lengths.to(device_cpu))\n",
        "        (hidden_state, cell_state) = hidden\n",
        "        decoder_hidden = (hidden_state[:self.decoder.num_layers], cell_state[:self.decoder.num_layers])\n",
        "\n",
        "        beam = [([SOS_token], 0)] # brackets allowing us to unpack both variables. Else python will consider it as 1 variable.\n",
        "        for _ in range(MAX_LENGTH):\n",
        "            candidates = []\n",
        "            for sequence, score in beam:\n",
        "                last_token = sequence[-1]\n",
        "\n",
        "                if last_token == EOS_token:\n",
        "                   candidates.append((sequence, score))\n",
        "                   continue\n",
        "\n",
        "                decoder_output, decoder_hidden = self.decoder(decoder_input, encoder_output, decoder_hidden)\n",
        "\n",
        "                probabilities = decoder_output.squeeze(0) / self.temperature\n",
        "\n",
        "                topk_probs, topk_indices = torch.topk(probabilities, self.beam_width)\n",
        "\n",
        "                for probability, index in zip(topk_probs.tolist(), topk_indices.tolist()):\n",
        "                    penalty_score = self.repetition(sequence + [index])\n",
        "                    candidates.append((sequence + [index], score + probability - penalty_score))\n",
        "\n",
        "            beam = sorted(candidates, key= lambda x: x[1], reverse=True)[:self.beam_width]\n",
        "        predicted_sentence = [self.libra.index2word[index] for index in beam[0][0] if index < self.libra.num_words]\n",
        "        return predicted_sentence\n",
        "\n",
        "\n",
        "def evaluateInput(decoder, encoder, searcher, libra):\n",
        "    while(1):\n",
        "        try:\n",
        "            input_sentence = input('User > ')\n",
        "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
        "            input_sentence = normalizeString(input_sentence)\n",
        "            output_words = searcher(input_sentence)\n",
        "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'SOS')]\n",
        "            print('Cleopatra:', ' '.join(output_words))\n",
        "        except KeyError:\n",
        "            print(\"Error: Encountered unknown word.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIU-IR9BzVUH"
      },
      "outputs": [],
      "source": [
        "libra = Library()\n",
        "\n",
        "task = \"train\"\n",
        "model_name = 'Cleopatra_model#v1.3.1'\n",
        "checkpoint=None\n",
        "start_model = \"yes\"\n",
        "loadFilename = None if start_model == \"no\" else \"/content/drive/MyDrive/data/save/Cleopatra_model#v1.3.1/Words-5000/2000_checkpoint.tar\"\n",
        "\n",
        "clip = 5.0\n",
        "n_iteration = 2000\n",
        "print_every = 1\n",
        "save_every = 2000\n",
        "times = 10\n",
        "\n",
        "if loadFilename:\n",
        "    print(\"Set to: 'trained model'\")\n",
        "    checkpoint = torch.load(loadFilename, map_location=device)\n",
        "    encoder_sd = checkpoint['en']\n",
        "    decoder_sd = checkpoint['de']\n",
        "    encoder_optimizer_sd = checkpoint['en_opt']\n",
        "    decoder_optimizer_sd = checkpoint['de_opt']\n",
        "    libra.__dict__ = checkpoint['voc_dict']\n",
        "    embedding_sd = checkpoint['embedding']\n",
        "    print(\"Loss: \",checkpoint[\"loss\"])\n",
        "    print(\"Time: \",checkpoint[\"time\"])\n",
        "else:\n",
        "    print(\"Set to: 'new model'\")\n",
        "\n",
        "if task == \"train\":\n",
        "    #if panda, change to 1.\n",
        "    #if csv, change to 2.\n",
        "    #if txt, change to 3.\n",
        "    #if custom, change to 4.\n",
        "    #if yml, change to 5.\n",
        "    #if all, change to any but 12345.\n",
        "    pairs = loadPrepareData(libra, 1)\n",
        "\n",
        "\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 4\n",
        "embedding_size = 512\n",
        "\n",
        "dropout = 0\n",
        "batch_size = 1\n",
        "learning_rate = 0.0000001\n",
        "prediction_size = 5000 #libra.num_words\n",
        "\n",
        "\n",
        "embedding = nn.Embedding(prediction_size, embedding_size)\n",
        "decoder = Decoder(embedding, embedding_size, dropout, decoder_n_layers, prediction_size)\n",
        "encoder = Encoder(embedding, embedding_size, dropout, encoder_n_layers)\n",
        "\n",
        "\n",
        "if loadFilename:\n",
        "    embedding.load_state_dict(embedding_sd)\n",
        "\n",
        "if loadFilename:\n",
        "    encoder.load_state_dict(encoder_sd)\n",
        "    decoder.load_state_dict(decoder_sd)\n",
        "\n",
        "if task == \"train\":\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    embedding = embedding.to(device)\n",
        "    encoder = encoder.to(device)\n",
        "    decoder = decoder.to(device)\n",
        "else:\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    embedding = embedding.to(device_cpu)\n",
        "    encoder = encoder.to(device_cpu)\n",
        "    decoder = decoder.to(device_cpu)\n",
        "\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-8)\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-8)\n",
        "\n",
        "if loadFilename:\n",
        "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
        "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
        "\n",
        "if task == \"train\":\n",
        "    trainIters(model_name, libra, save_dir, n_iteration, batch_size, checkpoint, clip,\n",
        "               print_every, save_every, loadFilename, prediction_size, decoder, encoder, dropout,\n",
        "               decoder_optimizer, encoder_optimizer, embedding, embedding_size, pairs, times)\n",
        "#Should later check if the 5000 predicions made are always the same words or they are random each time.\n",
        "if task == \"test\":\n",
        "    beam_width = 30\n",
        "    #searcher = BeamSearch(beam_width, encoder, decoder, libra)\n",
        "    searcher = GreedySearchDecoder(encoder, decoder, libra)\n",
        "    evaluateInput(decoder, encoder, searcher, libra)\n",
        "#Minimum label: 0\n",
        "#Maximum label: 24137"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCoyyP4Xu1Mj"
      },
      "outputs": [],
      "source": [
        "#Gradient_decent: Is used to tweak / change the weights and biases of the model to get closer to the appropiate behaviour.\n",
        "\n",
        "#AI: They work by knowing / recognising the answer to a quiestion.\n",
        "#For example; If I want an AI to guess the value of x through this operation (5 - x = 1) the AI recognises that x is 4.\n",
        "#Is not because because it knows the formula 5 - 4 = 1. Is becauses it recognized this pattern \"4 - 5 = 1\" thorough training.\n",
        "#Not because it knows the formula explicitly for example if I where to give it (5 - x = 2) it would not be able to guess what x is.\n",
        "#Because it does not know how to \"think_logically\" with things that it was not trained on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVhdxzumeN_D"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Artificial_Neurons = Artificial_Neurons are a type of function for Neural Networks.\n",
        "                     There are 2 types of neurons linear functions and non-linear functions.\n",
        "\n",
        "                     The number of inputs a neuron can take can be changed. Because we start of\n",
        "                     with a fixed value e.g. 512. But this can be modified during process.\n",
        "                     That means, it represents (n-elements: variable number of inputs).\n",
        "                     Mathematically this is represented as f(x1,..., x(n-element)) #Note: We add n-element there because it represents the number of inputs,\n",
        "                                                                                    therefor the last number. Where f is an activation function which introduces\n",
        "                                                                                    non-linearity. Which allows the model to learn and represent complex patterns.\n",
        "\n",
        "So the coded neuron functions would look like this:\n",
        "(non-linear neurons) #Note: They are non-linear because of max(0,z) meaning that if the\n",
        "                            resulting number is positive the output will be equalt to z\n",
        "                            but if it's negative the output will be 0. Is non-linear\n",
        "                            because it does not follow a straight path. A linear neuron\n",
        "                            will ALWAYS output the same as the input.\n",
        "\n",
        "def f(x, y):\n",
        "    return max(0,2*x + 3*y - 3) #Note: for 2 inputs\n",
        "\n",
        "#Note: This is called \"Unary non linear function\". Because it only takes 1 input.\n",
        "def f(x):\n",
        "    return max(0, 2*x - 3)\n",
        "\n",
        "Other functions:\n",
        "Quadratic function f(x) = a*x^2 +b*x +c\n",
        "Exponential function f(x) = e^x\n",
        "Logarithmic function f(x) = log(x)\n",
        "Sigmoid function f(x) = 1|1+e^-z\n",
        "\n",
        "                     A neuron can be split into 2 sub-functions such as:\n",
        "                     f(x1,..., x(n-element)) = h(g(x1,..., x(n-element)))\n",
        "\n",
        "                     Think of it as g is you linear function then h is you\n",
        "                     non-linear function. Resulting in 1 a neuron. The neuron\n",
        "                     architecture depends on how you want to make it. Is not\n",
        "                     always 1 function = 1 neuron.\n",
        "\n",
        "                     A linear function is represented as:\n",
        "                     g(x1,..., x(n-element)) = w1*x1 + ..., w(n-element)*x(n-element) + b\n",
        "                     w1 ..., w(n-element) and b are all parameters from the function.\n",
        "                     Thus, diffrent linear functions diffrent parameters too.\n",
        "\n",
        "                     Softmax is a special case. Since is not an activation function like\n",
        "                     a linear function is. It's output are probabilities of a categorical\n",
        "                     distribution.\n",
        "\n",
        "                     A neural networks needs to be composed of linear and non linear functions\n",
        "                     for example if we have a neural network of only linear functions. Then all\n",
        "                     we did is a ternary linear function, in simple a large linear function.\n",
        "                     Thus, a neural network needs linear and non linear funcions to predict the\n",
        "                     final output as accurate as possible.\n",
        "\n",
        "                     A complete nuron is basically a linear and a non linear activation function.\n",
        "                     Making it more powerful when it comes to accurate when producing values.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Chain Rule in Calculus: The chain rule is a fundamental principle in calculus used for differentiating composite functions. If you have a function composed of two or more functions, say y = f(g(x)), the derivative of y with respect to x is given by the product of the derivative of f with respect to g(x) and the derivative of g(x) with respect to x. Mathematically, this is expressed as dy/dx = (df/dg) * (dg/dx).\n",
        "Backpropagation and the Chain Rule: Backpropagation uses the chain rule to compute the gradient of the loss function with respect to the weights and biases. It starts from the output layer and works backwards through each layer, hence the name “backpropagation”. At each layer, it computes the local gradient and then multiplies it with the gradient flowing into that layer from the next layer (closer to the output). This is essentially applying the chain rule.\n",
        "Where is the Gradient Computed?: The gradient is computed at each neuron for its weights and bias. For each weight, the gradient tells us how much a small change in that weight would affect the overall loss. These gradients are stored and used to update the weights and biases in the direction that reduces the loss.\n",
        "So, in a literal sense, the gradient is computed and stored in each neuron for each of its weights and biases. If you’re using a deep learning framework like TensorFlow or PyTorch, this is handled automatically when you call the backward() function. The gradients are stored in a variable associated with each weight and bias, and can be accessed after the backward pass.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QgaGtsPUBlrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#Inteligence, Knowledge, Brain, Mind, Cognition, Calculation, Logic\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "CRAOAppobSsa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}